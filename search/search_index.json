{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Learning Collection Site \u00b6 Just my collection all I learned during the work time and other times.","title":"Home"},{"location":"#learning-collection-site","text":"Just my collection all I learned during the work time and other times.","title":"Learning Collection Site"},{"location":"aws-acd/","text":"AWS Certifed Developer - Associate (DVA -C01) Cert Guide \u00b6","title":"Book Home"},{"location":"aws-acd/#aws-certifed-developer-associate-dva-c01-cert-guide","text":"","title":"AWS Certifed Developer - Associate (DVA -C01) Cert Guide"},{"location":"aws-acd/chapter-1/","text":"Chapter 1 Overview of AWS \u00b6 QUIZ \u00b6 Which of these is not a feature of cloud computing, as defined by the National Institute of Standards and Technology (NIST)? Rapid elasticity Self-service capability Application agility Broad network access Which of the following is AWS responsible for? Updating the EC2 operating system Securing the hypervisor Encrypting RDS databases Configuring security group rules Which of these services is designed with unlimited storage capacity? RDS S3 EBS An EC2 instance store volume When is an RDS database considered highly available? Always. An RDS database is inherently highly available. Only when an RDS database is replicated to another database on the same instance. Only when an RDS database has replicas in the same availability zone. Only when an RDS database has replicas in two availability zones. When accessing AWS, which of these is true? All calls to AWS are API calls, regardless of the access method. Calls through the CLI are API calls, whereas calls through the Management Console and the SDK are direct invocations. Calls through the CLI and the SDKs are API calls, whereas calls through the Management Console are direct invocations. Calls through the SDK are API calls, whereas calls through the Management Console and the CLI are direct invocations. Which of these services gives you the ability to run virtual machines in the cloud? VMM Systems Manager Lambda EC2 Which of these responsibilities are customers responsible for? (Choose two.) Creating and managing SSH keys Securing the hypervisor Enabling HTTPS on the Apache server Decommissioning storage devices Which of the following most accurately describes IaaS in the cloud? Compute, networking, storage, and platforms in the datacenter Compute, networking, and storage solutions across a VPN Compute, networking, and storage solutions as a service Compute, networking, storage, and platforms as a service You can access AWS services through the API by using the following credentials: An IAM-generated SSH secret key and public key An IAM-generated username and password An IAM-generated API secret key and API key An IAM-generated secret key and an access key ID What is the smallest component of the AWS global architecture? A hypervisor A datacenter An availability zone A region Foundation Topics \u00b6 NIST \u00b6 NIST defines three delivery models that indicate how a service is consumed and determine the level of interaction the user has with the underlying compute services: IaaS: Infrastructure as a Service PaaS: Platform as a Service SaaS: Software as a Service Levels of shared responsibility in IaaS, PaaS, and SaaS \u00b6 Infrastructure as a Service (IaaS) The provider is responsible for Securing the hardware in the datacenter Securing the hypervisor Securing the storage subsystems Securing the physical network devices in the datacenter Securing the uplink to the Internet and the uplinks between the datacenters The consumer is responsible for Securing the operating system user and network access (firewall, users, ports, key pairs, and so on) Ensuring that the operating system and application are updated Deploying and managing the database application Securing the database application from unauthorized access Securing the database content from unauthorized access Platform as a Service (PaaS)) The provider is responsible for Securing the hardware in the datacenter Securing the hypervisor Securing the storage subsystems Securing the physical network devices in the datacenter Securing the uplink to the Internet and the uplinks between the datacenters Securing the operating system user and network access (firewall, users, ports, key pairs, and so on) Ensuring that the operating system and application are updated Deploying and managing the database application Securing the database application from unauthorized access AWS SERVICES \u00b6 AWS defines two service types: the Foundation services and the Platform services. Foundation Services \u00b6 AWS Foundation services include all the IaaS services available in AWS and can be divided into several functional groups: Network services Compute services Storage services Security and identity services End-user applications Network Services \u00b6 The network services allow your application\u2019s components to interact with each other and also connect your application to the Internet and private networks. Examples of network services include the following: Amazon Virtual Private Cloud (VPC) : Allows you to connect your application with private network ranges, connect those private ranges with the Internet, and assign public IP addresses AWS Direct Connect : A private optical fiber connection service that connects your on-premises sites with AWS AWS Virtual Private Gateway : A component of VPC that provides the capability for establishing VPN connection with your on-premises sites Amazon Route 53 : The next-generation, API-addressable Domain Name Service (DNS) from AWS Amazon CloudFront : The caching and Content Delivery Network (CDN) service in the AWS cloud Amazon Elastic Load Balancing (ELB) : Allows load balancing of traffic across AWS Elastic Compute Cloud (EC2) instances, AWS Elastic Container Service (ECS) containers, or other IP addressable targets Compute Services \u00b6 You have a lot of flexibility when it comes to compute services in AWS. The following are examples of compute offerings in AWS: Amazon Elastic Cloud Computing (EC2) : Provides the ability to deploy and operate virtual machines running Linux and Windows in the AWS cloud Amazon Elastic Container Service (ECS) : Provides the ability to deploy, orchestrate, and operate containers in the AWS cloud Amazon Elastic Kubernetes Service (EKS) : Provides the ability to deploy, orchestrate, and operate Kubernetes clusters in the AWS cloud Amazon Lambda : Provides the ability to process simple functions in the AWS cloud Storage Services \u00b6 There are many types of data, and for each type you need to choose the right storage solution. In the AWS cloud, you have several different storage options, depending on the types of data you are storing. Here are a few examples: Amazon Elastic Block Storage (EBS) : EBS provides block-accessible, network attached, persistent storage for volumes that you can connect to EC2 instances and ECS containers. Amazon Elastic File System (EFS) : EFS provides a network attached file system that supports the Linux Network File System protocol (NFS) and allows you to share files among EC2 instances, ECS containers, and other services. Amazon Simple Storage Service (S3) : Designed to store unlimited amounts of data, S3 is the ultimate object storage system. All objects in S3 are accessible via standard HTTP methods. Amazon Glacier : This archiving storage solution can be automatically integrated with S3. AWS Storage Gateway : This hybrid storage solution exposes AWS as a storage back end to your on-premises servers. AWS SnowBall and SnowMobile : These data transfer devices allow for physically moving data from on premises to the cloud at any scale Quiz Answer \u00b6 C. Application agility B. Securing the hpervisor B. S3 D.Only when an RDS database has replicas in two availability zones. A. All calls to AWS are API calls, regardless of the access method. D. EC2 A. Creating and managing SSH keys C. Enabling HTTPS on the Apache server C. Compute, networking, and storage solutions as a service D. An IAM-generated secret key and an access key ID C. A datacenter Q & A \u00b6 Complete this sentence: A fault isolation environment that is composed from one or more datacenters in AWS is called a(n) availability zone Complete this sentence: To replicate a corporate network, a cloud customer would use the IaaS service model, as defined by NIST. Complete this sentence: Route 53 is the next-generation DNS service available from AWS. What is the most important security recommendation when opening an AWS account? Answer : Stop using the root account for everyday tasks. What type of access to AWS can you gain with a secret key and an access key ID? Answer : Programmatic access What happens when you run the help command in the CLI? Answer : The man page of the AWS CLI, the service, or the operation is displayed. What are the three components of the AWS CLI model? Answer : The service, operation, and options Name two API method types are available in some SDKs. Answer : Client and resource APIs What does the following command do? aws ec2 describe-instances --profile developer01 Answer : It runs a describe-instances operation on the EC2 service with the credentials and configuration defined in the developer01 profile under the developer01 section in the files. What option can you use to override the default output of the AWS CLI? Answer : \u2013output text|json|table","title":"Chapter 1"},{"location":"aws-acd/chapter-1/#chapter-1-overview-of-aws","text":"","title":"Chapter 1 Overview of AWS"},{"location":"aws-acd/chapter-1/#quiz","text":"Which of these is not a feature of cloud computing, as defined by the National Institute of Standards and Technology (NIST)? Rapid elasticity Self-service capability Application agility Broad network access Which of the following is AWS responsible for? Updating the EC2 operating system Securing the hypervisor Encrypting RDS databases Configuring security group rules Which of these services is designed with unlimited storage capacity? RDS S3 EBS An EC2 instance store volume When is an RDS database considered highly available? Always. An RDS database is inherently highly available. Only when an RDS database is replicated to another database on the same instance. Only when an RDS database has replicas in the same availability zone. Only when an RDS database has replicas in two availability zones. When accessing AWS, which of these is true? All calls to AWS are API calls, regardless of the access method. Calls through the CLI are API calls, whereas calls through the Management Console and the SDK are direct invocations. Calls through the CLI and the SDKs are API calls, whereas calls through the Management Console are direct invocations. Calls through the SDK are API calls, whereas calls through the Management Console and the CLI are direct invocations. Which of these services gives you the ability to run virtual machines in the cloud? VMM Systems Manager Lambda EC2 Which of these responsibilities are customers responsible for? (Choose two.) Creating and managing SSH keys Securing the hypervisor Enabling HTTPS on the Apache server Decommissioning storage devices Which of the following most accurately describes IaaS in the cloud? Compute, networking, storage, and platforms in the datacenter Compute, networking, and storage solutions across a VPN Compute, networking, and storage solutions as a service Compute, networking, storage, and platforms as a service You can access AWS services through the API by using the following credentials: An IAM-generated SSH secret key and public key An IAM-generated username and password An IAM-generated API secret key and API key An IAM-generated secret key and an access key ID What is the smallest component of the AWS global architecture? A hypervisor A datacenter An availability zone A region","title":"QUIZ"},{"location":"aws-acd/chapter-1/#foundation-topics","text":"","title":"Foundation Topics"},{"location":"aws-acd/chapter-1/#nist","text":"NIST defines three delivery models that indicate how a service is consumed and determine the level of interaction the user has with the underlying compute services: IaaS: Infrastructure as a Service PaaS: Platform as a Service SaaS: Software as a Service","title":"NIST"},{"location":"aws-acd/chapter-1/#levels-of-shared-responsibility-in-iaas-paas-and-saas","text":"Infrastructure as a Service (IaaS) The provider is responsible for Securing the hardware in the datacenter Securing the hypervisor Securing the storage subsystems Securing the physical network devices in the datacenter Securing the uplink to the Internet and the uplinks between the datacenters The consumer is responsible for Securing the operating system user and network access (firewall, users, ports, key pairs, and so on) Ensuring that the operating system and application are updated Deploying and managing the database application Securing the database application from unauthorized access Securing the database content from unauthorized access Platform as a Service (PaaS)) The provider is responsible for Securing the hardware in the datacenter Securing the hypervisor Securing the storage subsystems Securing the physical network devices in the datacenter Securing the uplink to the Internet and the uplinks between the datacenters Securing the operating system user and network access (firewall, users, ports, key pairs, and so on) Ensuring that the operating system and application are updated Deploying and managing the database application Securing the database application from unauthorized access","title":"Levels of shared responsibility in IaaS, PaaS, and SaaS"},{"location":"aws-acd/chapter-1/#aws-services","text":"AWS defines two service types: the Foundation services and the Platform services.","title":"AWS SERVICES"},{"location":"aws-acd/chapter-1/#foundation-services","text":"AWS Foundation services include all the IaaS services available in AWS and can be divided into several functional groups: Network services Compute services Storage services Security and identity services End-user applications","title":"Foundation Services"},{"location":"aws-acd/chapter-1/#network-services","text":"The network services allow your application\u2019s components to interact with each other and also connect your application to the Internet and private networks. Examples of network services include the following: Amazon Virtual Private Cloud (VPC) : Allows you to connect your application with private network ranges, connect those private ranges with the Internet, and assign public IP addresses AWS Direct Connect : A private optical fiber connection service that connects your on-premises sites with AWS AWS Virtual Private Gateway : A component of VPC that provides the capability for establishing VPN connection with your on-premises sites Amazon Route 53 : The next-generation, API-addressable Domain Name Service (DNS) from AWS Amazon CloudFront : The caching and Content Delivery Network (CDN) service in the AWS cloud Amazon Elastic Load Balancing (ELB) : Allows load balancing of traffic across AWS Elastic Compute Cloud (EC2) instances, AWS Elastic Container Service (ECS) containers, or other IP addressable targets","title":"Network Services"},{"location":"aws-acd/chapter-1/#compute-services","text":"You have a lot of flexibility when it comes to compute services in AWS. The following are examples of compute offerings in AWS: Amazon Elastic Cloud Computing (EC2) : Provides the ability to deploy and operate virtual machines running Linux and Windows in the AWS cloud Amazon Elastic Container Service (ECS) : Provides the ability to deploy, orchestrate, and operate containers in the AWS cloud Amazon Elastic Kubernetes Service (EKS) : Provides the ability to deploy, orchestrate, and operate Kubernetes clusters in the AWS cloud Amazon Lambda : Provides the ability to process simple functions in the AWS cloud","title":"Compute Services"},{"location":"aws-acd/chapter-1/#storage-services","text":"There are many types of data, and for each type you need to choose the right storage solution. In the AWS cloud, you have several different storage options, depending on the types of data you are storing. Here are a few examples: Amazon Elastic Block Storage (EBS) : EBS provides block-accessible, network attached, persistent storage for volumes that you can connect to EC2 instances and ECS containers. Amazon Elastic File System (EFS) : EFS provides a network attached file system that supports the Linux Network File System protocol (NFS) and allows you to share files among EC2 instances, ECS containers, and other services. Amazon Simple Storage Service (S3) : Designed to store unlimited amounts of data, S3 is the ultimate object storage system. All objects in S3 are accessible via standard HTTP methods. Amazon Glacier : This archiving storage solution can be automatically integrated with S3. AWS Storage Gateway : This hybrid storage solution exposes AWS as a storage back end to your on-premises servers. AWS SnowBall and SnowMobile : These data transfer devices allow for physically moving data from on premises to the cloud at any scale","title":"Storage Services"},{"location":"aws-acd/chapter-1/#quiz-answer","text":"C. Application agility B. Securing the hpervisor B. S3 D.Only when an RDS database has replicas in two availability zones. A. All calls to AWS are API calls, regardless of the access method. D. EC2 A. Creating and managing SSH keys C. Enabling HTTPS on the Apache server C. Compute, networking, and storage solutions as a service D. An IAM-generated secret key and an access key ID C. A datacenter","title":"Quiz Answer"},{"location":"aws-acd/chapter-1/#q-a","text":"Complete this sentence: A fault isolation environment that is composed from one or more datacenters in AWS is called a(n) availability zone Complete this sentence: To replicate a corporate network, a cloud customer would use the IaaS service model, as defined by NIST. Complete this sentence: Route 53 is the next-generation DNS service available from AWS. What is the most important security recommendation when opening an AWS account? Answer : Stop using the root account for everyday tasks. What type of access to AWS can you gain with a secret key and an access key ID? Answer : Programmatic access What happens when you run the help command in the CLI? Answer : The man page of the AWS CLI, the service, or the operation is displayed. What are the three components of the AWS CLI model? Answer : The service, operation, and options Name two API method types are available in some SDKs. Answer : Client and resource APIs What does the following command do? aws ec2 describe-instances --profile developer01 Answer : It runs a describe-instances operation on the EC2 service with the credentials and configuration defined in the developer01 profile under the developer01 section in the files. What option can you use to override the default output of the AWS CLI? Answer : \u2013output text|json|table","title":"Q &amp; A"},{"location":"aws-acd/chapter-2/","text":"Chapter 2 Authentication, Identity, and Access Management \u00b6 QUIZ \u00b6 When using IAM, to which of the following IAM principles can you assign long-term credentials? User Group Role Policy When delivering federated access to an application, how does the user gain access to AWS services? Via delegated policy based on the user\u2019s group or pool membership Via role assumption based on the user\u2019s group or pool membership Via federated policy based on the user\u2019s group or pool membership Via shared role based on the user\u2019s group or pool membership When assigning strict permissions for accessing S3 to a mobile application, how could you lock down the permissions to exactly one action? Allow the action in the bucket policy for all users. All other actions are implicitly denied. Allow the action in the bucket policy for all users. Explicitly deny all other actions by using a \u201cNot\u201d condition. Allow the action in the role policy. All other actions are implicitly denied. Allow the action in the role policy. Explicitly deny all other actions by using a \u201cNot\u201d condition. Encryption in transit can be achieved through which of the following? TLS Client-side encryption IPsec VPN All of these answers are correct. You are writing code for a mobile application. You need to allow the application to access an S3 bucket. What would be the most secure way to access the AWS resource? Create an ACL and apply it to the bucket. Create a bucket policy that allows the ACL access to the bucket. Create an IdP that authenticates the user of the application and provides it with a role. Create a bucket policy that allows the role access to the bucket. Create an IdP that authenticates the user of the application and provides it with a role. Create an IAM policy with access to S3 and attach it to the role. Authenticate the user of the application through IAM and provide it with a role. Create an IAM policy with access to S3 and attach it to the role. To allow an application on an EC2 instance to use S3, which of the following is the safest way to distribute credentials? Using secret access key and access key ID in the ./aws/credentials file Storing credentials in the code Using the environment variables Using a role For which of the following can you use IAM? (Choose all that apply.) Authenticating and authorizing access to the AWS Management Console Authenticating and authorizing access to RDS databases Authenticating and authorizing access to EC2 operating systems All of the above What is the maximum number of IAM users? Unlimited 10,000 5000 50,000 Which service encrypts all data at rest by default? S3 RDS Glacier SQS Foundation Topics \u00b6 IDENTITY PRINCIPALS IN IAM \u00b6 Two types of actions can be allowed over AWS resources via a policy: Management of AWS resources : This type of permission allows users to read the state or change and delete resources created in AWS. For example, a user might have permissions to create and manage S3 buckets, change configurations, and delete configurations. Management permissions can be granted over any AWS service. Access to the contents of an AWS resource : This type of permission can allow access to the content held in an AWS resource through IAM authentication and authorization. With it, you can allow access to the contents of a database or a storage layer, access to post or read from a service, and so on. For example, you can allow a user to create, update, or read items in a DynamoDB database. Access permissions are not available for all AWS resources because some of the resources cannot be IAM integrated. An example of a resource where access permissions cannot be granted is an EC2 instance operating system. Access Keys, Secret Keys, and Passwords \u00b6 Two types of credentials can be created for a user: Password : In combination with a username and an account ID, a password can be used to authenticate against the AWS Management Console. If the user requires access to the console to manage resources from the AWS GUI, a password should be assigned to the user. Once a user is authenticated, a temporary token is generated that is used by the web interface so that the user does not need to continuously enter the username and password for each and every request. Access key ID and secret access key : This pair of credentials allows for direct access to the AWS APIs or use of the AWS CLI and the AWS SDKs to perform actions against AWS resources. These two credentials can be used to call up AWS resources and sign each request being sent to the API. When using the CLI and SDKs, the signing of each request to the AWS APIs is built in. Quiz Answer \u00b6 A.User B.Via role assumption based on the user\u2019s group or pool membership D. Allow the action in the role policy. Explicitly deny all other actions by using a \u201cNot\u201d condition. D. All of these answers are correct B. Create an IdP that authenticates the user of the application and provides it with a role. Create a bucket policy that allows the role access to the bucket. D. Using a role A. Authenticating and authorizing access to the AWS Management Console B. Authenticating and authorizing access to RDS databases C. 5000 C. Glacier A. Users Q&A \u00b6 True or false: An AWS user can have both a username and password and one or two secret keys and access key IDs assigned. Answer : True What security principal does not have credentials directly assigned to it? Answer : An IAM Group Complete this sentence: An IAM Role can be assigned to an AWS service to allow it to access other AWS services. If a user is a member of a group that denies that user access to S3, can you grant access to a specific bucket by assigning an inline policy to the user? Answer : No. All policies combine with a logical AND. A deny in one policy will take precedence over any allow. What is a web identity? Answer : An external directory accessible on the web When federating IAM with an IDP, how are permissions granted to the user? Answer : THrough a role that is tied to the idenity of the user by group or pool membership Complete this sentence: One of the benefits of federation with IAM is that you can support an unlimited number any number of users. True or false: By default, a Lambda function can access all the resources within your AWS account in an unrestricted manner. Answer :False. Lambda needs to be granted permissions to access AWS resources within your account What types of encryption would you recommend for a MySQL database when end-to-end encryption is required? Answer : Encryption in transit via TLS and encryption of data at rest via data volume encryption When connecting to an EC2 instance in AWS, what encryption options would you be able to use? Answer : Client-side encryption; encryption of traffic with SSL, TLS and so on; and a VPN would all provide encryption between the client and the EC2 instance.","title":"Chapter 2"},{"location":"aws-acd/chapter-2/#chapter-2-authentication-identity-and-access-management","text":"","title":"Chapter 2 Authentication, Identity, and Access Management"},{"location":"aws-acd/chapter-2/#quiz","text":"When using IAM, to which of the following IAM principles can you assign long-term credentials? User Group Role Policy When delivering federated access to an application, how does the user gain access to AWS services? Via delegated policy based on the user\u2019s group or pool membership Via role assumption based on the user\u2019s group or pool membership Via federated policy based on the user\u2019s group or pool membership Via shared role based on the user\u2019s group or pool membership When assigning strict permissions for accessing S3 to a mobile application, how could you lock down the permissions to exactly one action? Allow the action in the bucket policy for all users. All other actions are implicitly denied. Allow the action in the bucket policy for all users. Explicitly deny all other actions by using a \u201cNot\u201d condition. Allow the action in the role policy. All other actions are implicitly denied. Allow the action in the role policy. Explicitly deny all other actions by using a \u201cNot\u201d condition. Encryption in transit can be achieved through which of the following? TLS Client-side encryption IPsec VPN All of these answers are correct. You are writing code for a mobile application. You need to allow the application to access an S3 bucket. What would be the most secure way to access the AWS resource? Create an ACL and apply it to the bucket. Create a bucket policy that allows the ACL access to the bucket. Create an IdP that authenticates the user of the application and provides it with a role. Create a bucket policy that allows the role access to the bucket. Create an IdP that authenticates the user of the application and provides it with a role. Create an IAM policy with access to S3 and attach it to the role. Authenticate the user of the application through IAM and provide it with a role. Create an IAM policy with access to S3 and attach it to the role. To allow an application on an EC2 instance to use S3, which of the following is the safest way to distribute credentials? Using secret access key and access key ID in the ./aws/credentials file Storing credentials in the code Using the environment variables Using a role For which of the following can you use IAM? (Choose all that apply.) Authenticating and authorizing access to the AWS Management Console Authenticating and authorizing access to RDS databases Authenticating and authorizing access to EC2 operating systems All of the above What is the maximum number of IAM users? Unlimited 10,000 5000 50,000 Which service encrypts all data at rest by default? S3 RDS Glacier SQS","title":"QUIZ"},{"location":"aws-acd/chapter-2/#foundation-topics","text":"","title":"Foundation Topics"},{"location":"aws-acd/chapter-2/#identity-principals-in-iam","text":"Two types of actions can be allowed over AWS resources via a policy: Management of AWS resources : This type of permission allows users to read the state or change and delete resources created in AWS. For example, a user might have permissions to create and manage S3 buckets, change configurations, and delete configurations. Management permissions can be granted over any AWS service. Access to the contents of an AWS resource : This type of permission can allow access to the content held in an AWS resource through IAM authentication and authorization. With it, you can allow access to the contents of a database or a storage layer, access to post or read from a service, and so on. For example, you can allow a user to create, update, or read items in a DynamoDB database. Access permissions are not available for all AWS resources because some of the resources cannot be IAM integrated. An example of a resource where access permissions cannot be granted is an EC2 instance operating system.","title":"IDENTITY PRINCIPALS IN IAM"},{"location":"aws-acd/chapter-2/#access-keys-secret-keys-and-passwords","text":"Two types of credentials can be created for a user: Password : In combination with a username and an account ID, a password can be used to authenticate against the AWS Management Console. If the user requires access to the console to manage resources from the AWS GUI, a password should be assigned to the user. Once a user is authenticated, a temporary token is generated that is used by the web interface so that the user does not need to continuously enter the username and password for each and every request. Access key ID and secret access key : This pair of credentials allows for direct access to the AWS APIs or use of the AWS CLI and the AWS SDKs to perform actions against AWS resources. These two credentials can be used to call up AWS resources and sign each request being sent to the API. When using the CLI and SDKs, the signing of each request to the AWS APIs is built in.","title":"Access Keys, Secret Keys, and Passwords"},{"location":"aws-acd/chapter-2/#quiz-answer","text":"A.User B.Via role assumption based on the user\u2019s group or pool membership D. Allow the action in the role policy. Explicitly deny all other actions by using a \u201cNot\u201d condition. D. All of these answers are correct B. Create an IdP that authenticates the user of the application and provides it with a role. Create a bucket policy that allows the role access to the bucket. D. Using a role A. Authenticating and authorizing access to the AWS Management Console B. Authenticating and authorizing access to RDS databases C. 5000 C. Glacier A. Users","title":"Quiz Answer"},{"location":"aws-acd/chapter-2/#qa","text":"True or false: An AWS user can have both a username and password and one or two secret keys and access key IDs assigned. Answer : True What security principal does not have credentials directly assigned to it? Answer : An IAM Group Complete this sentence: An IAM Role can be assigned to an AWS service to allow it to access other AWS services. If a user is a member of a group that denies that user access to S3, can you grant access to a specific bucket by assigning an inline policy to the user? Answer : No. All policies combine with a logical AND. A deny in one policy will take precedence over any allow. What is a web identity? Answer : An external directory accessible on the web When federating IAM with an IDP, how are permissions granted to the user? Answer : THrough a role that is tied to the idenity of the user by group or pool membership Complete this sentence: One of the benefits of federation with IAM is that you can support an unlimited number any number of users. True or false: By default, a Lambda function can access all the resources within your AWS account in an unrestricted manner. Answer :False. Lambda needs to be granted permissions to access AWS resources within your account What types of encryption would you recommend for a MySQL database when end-to-end encryption is required? Answer : Encryption in transit via TLS and encryption of data at rest via data volume encryption When connecting to an EC2 instance in AWS, what encryption options would you be able to use? Answer : Client-side encryption; encryption of traffic with SSL, TLS and so on; and a VPN would all provide encryption between the client and the EC2 instance.","title":"Q&amp;A"},{"location":"aws-acd/chapter-3/","text":"Chapter 3. Compute Services in AWS \u00b6 Quiz \u00b6 VPC A is peered to VPC B. VPC B is peered to VPC C. You have set up routing in VPC A, which lists the VPC C subnet as a subnet of VPC B. You are trying to ping an instance in VPC C from VPC A, but you are not getting a response. Why? Transient connections are not supported by VPC peering. Your security groups of the VPC C instance do not allow incoming pings. You need to create an NACL in VPC C that will allow pings from VPC A. The NAT service in VPC B is not configured correctly. You are tasked with migrating an EC2 instance from one availability zone to another. Which approach would be the best to achieve full data consistency? Shut down the instance. Then restart the instance and select the new availability zone. Keep the instance running. Select Migrate to AZ in the instance actions and select the new availability zone. Shut down the instance, create a snapshot, start a new instance from the snapshot, and select the new availability zone. Keep the instance running. Create a snapshot with the no-shutdown option. Start a new instance from the snapshot and select the new availability zone. You are required to select a storage location for your MySQL database server on an EC2 instance. What AWS service would be the most appropriate for such an object? RDS EBS EFS S3 With ECS, what allows you to control high availability of a containerized application? Placement of ECS tasks across ECS instances Placement of ECS tasks into an ECS cluster Placement of ECS instances across regions Placement of ECS instances across availability zones Which scripting languages are supported in a CloudFormation template? (Choose two.) YAML Ruby DSL JSON Python To set up a route from an on-premises location to a VPC subnet through Direct Connect, which of the following do you need to use? RIPv2 RIPv1 Static routing BGP To change the number of instances in an Auto Scaling group from 1 to 3, which count do you set to 3? Percentage Maximum instances Desired instances Running instances To maximize IOPS in an EBS volume, which of the following would you need to select? Provisioned IOPS volume General purpose volume Disk-backed volume Dedicated IOPS volume To automate the infrastructure deployment of a three-tier application, which of the following options could you use? (Choose all that apply.) CloudFormation CLI CloudTrail OpsWorks Stacks Which of the following compute options would be best suited for a tiny 100 MB microservices platform that needs to run in response to a user action? Lambda EC2 ECS EKS Foundation Topics \u00b6 Notes \u00b6 Modern networking requirements are typically divided into two categories: Local area network (LANs) : These are private networks that allow communication only within a certain limited set of network addresses (usually) within one organization. Wide area network (WANs) : These are either private or public networks that are designed to allow communication at a distance with multiple parties. When these networks are publc, the term WAN is usually replaced with the Internet Internet Protocal comes in two versions: version 4 (IPv4) and version 6 (IPv6) The IPv4 protocol has a 32-bit addressing field. The IPv6 protocol has a 128-bit addressing field. Networking in AWS \u00b6 The following are the most important networking tools available in AWS Amazon Virtual Private Cloud (VPC) : A service for creating logically isolated networks in the cloud VPC network ACLs and security groups : Tools for securing network and instance access in VPC AWS Direct Connect and VPC gateways : Tools for connecting your on-premises networks with AWS Amazon Route 53 : A next-generation DNS service with an innovative API that allow for programmatic access to the DNS services Amazon CloudFront : A dynamic caching and CDN service in the AWS cloud Amazon Elastic Load Balancing(ELB) : Load balancing as a service in the AWS cloud Amazon Web Application Firewall (WAF) : A tool that protects web applications from external attacks using exploits and security vulnerabilities AWS Shield : An AWS managed DDoS service Amazon Virtual Private Cloud (VPC) \u00b6 Quiz Answer \u00b6 A. Transient connections are not supported by VPC peering. C. Shut down the instance, create a snapshot, start a new instance from the snapshot, and select the new availability zone. B. EBS D. Placement of ECS instances across availability zones A. YAML C. JSON D. BGP C. Desired instances A. Provisioned IOPS volume A. CloudFormation B. CLI D OpsWorks Stacks A. Lambda","title":"Chapter 3"},{"location":"aws-acd/chapter-3/#chapter-3-compute-services-in-aws","text":"","title":"Chapter 3. Compute Services in AWS"},{"location":"aws-acd/chapter-3/#quiz","text":"VPC A is peered to VPC B. VPC B is peered to VPC C. You have set up routing in VPC A, which lists the VPC C subnet as a subnet of VPC B. You are trying to ping an instance in VPC C from VPC A, but you are not getting a response. Why? Transient connections are not supported by VPC peering. Your security groups of the VPC C instance do not allow incoming pings. You need to create an NACL in VPC C that will allow pings from VPC A. The NAT service in VPC B is not configured correctly. You are tasked with migrating an EC2 instance from one availability zone to another. Which approach would be the best to achieve full data consistency? Shut down the instance. Then restart the instance and select the new availability zone. Keep the instance running. Select Migrate to AZ in the instance actions and select the new availability zone. Shut down the instance, create a snapshot, start a new instance from the snapshot, and select the new availability zone. Keep the instance running. Create a snapshot with the no-shutdown option. Start a new instance from the snapshot and select the new availability zone. You are required to select a storage location for your MySQL database server on an EC2 instance. What AWS service would be the most appropriate for such an object? RDS EBS EFS S3 With ECS, what allows you to control high availability of a containerized application? Placement of ECS tasks across ECS instances Placement of ECS tasks into an ECS cluster Placement of ECS instances across regions Placement of ECS instances across availability zones Which scripting languages are supported in a CloudFormation template? (Choose two.) YAML Ruby DSL JSON Python To set up a route from an on-premises location to a VPC subnet through Direct Connect, which of the following do you need to use? RIPv2 RIPv1 Static routing BGP To change the number of instances in an Auto Scaling group from 1 to 3, which count do you set to 3? Percentage Maximum instances Desired instances Running instances To maximize IOPS in an EBS volume, which of the following would you need to select? Provisioned IOPS volume General purpose volume Disk-backed volume Dedicated IOPS volume To automate the infrastructure deployment of a three-tier application, which of the following options could you use? (Choose all that apply.) CloudFormation CLI CloudTrail OpsWorks Stacks Which of the following compute options would be best suited for a tiny 100 MB microservices platform that needs to run in response to a user action? Lambda EC2 ECS EKS","title":"Quiz"},{"location":"aws-acd/chapter-3/#foundation-topics","text":"","title":"Foundation Topics"},{"location":"aws-acd/chapter-3/#notes","text":"Modern networking requirements are typically divided into two categories: Local area network (LANs) : These are private networks that allow communication only within a certain limited set of network addresses (usually) within one organization. Wide area network (WANs) : These are either private or public networks that are designed to allow communication at a distance with multiple parties. When these networks are publc, the term WAN is usually replaced with the Internet Internet Protocal comes in two versions: version 4 (IPv4) and version 6 (IPv6) The IPv4 protocol has a 32-bit addressing field. The IPv6 protocol has a 128-bit addressing field.","title":"Notes"},{"location":"aws-acd/chapter-3/#networking-in-aws","text":"The following are the most important networking tools available in AWS Amazon Virtual Private Cloud (VPC) : A service for creating logically isolated networks in the cloud VPC network ACLs and security groups : Tools for securing network and instance access in VPC AWS Direct Connect and VPC gateways : Tools for connecting your on-premises networks with AWS Amazon Route 53 : A next-generation DNS service with an innovative API that allow for programmatic access to the DNS services Amazon CloudFront : A dynamic caching and CDN service in the AWS cloud Amazon Elastic Load Balancing(ELB) : Load balancing as a service in the AWS cloud Amazon Web Application Firewall (WAF) : A tool that protects web applications from external attacks using exploits and security vulnerabilities AWS Shield : An AWS managed DDoS service","title":"Networking in AWS"},{"location":"aws-acd/chapter-3/#amazon-virtual-private-cloud-vpc","text":"","title":"Amazon Virtual Private Cloud (VPC)"},{"location":"aws-acd/chapter-3/#quiz-answer","text":"A. Transient connections are not supported by VPC peering. C. Shut down the instance, create a snapshot, start a new instance from the snapshot, and select the new availability zone. B. EBS D. Placement of ECS instances across availability zones A. YAML C. JSON D. BGP C. Desired instances A. Provisioned IOPS volume A. CloudFormation B. CLI D OpsWorks Stacks A. Lambda","title":"Quiz Answer"},{"location":"aws-acd/chapter-4/","text":"Chapter 4 Storing Data in AWS \u00b6 STORING STATIC ASSETS IN AWS \u00b6 Amazon S3 \u00b6 Two different ways of allowing access to S3 bucket Use an access control list (ACL) or a bucket ACL: quickly allow access to a large group of users, such as another account or everyone with a specific type of access to all the keys in the bucket. Use a bucket policy: A bucket policy can be used to granularly control access to a bucket and its contents. Working with S3 in the AWS CLI \u00b6 AWS create-bucket aws s3api create - bucket -- bucket bucket - name -- region region - id aws s3 cp / my - website / s3: //everyonelovesaws/ --recursive -- exclude \"*\" -- include \"*.html\" Access content within a bucket on S3 http { s }: // s3 . { region-id } . amazonaws . com / { bucket-name } / { optional key prefix } / { key-name } Bucket names are globally unique. Because every bucket name is essentially a subdomain of .s3.amazonaws.com , there is no way to make two buckets with the same name in all of AWS. Securing a static website through HTTPS with a free certificate attached to a CloudFront distribution. There are three status options: Disabled, Enabled, and Suspended. By default, a bucket has versioning disabled, but once it is enabled, it cannot be removed but only suspended. S3 Storage Tiers \u00b6 s3 has six storage classes: S3 Standard : General-purpose online storage with 99.99% availability and 99.999999999% durability (that is, \u201c11 9s\u201d). S3 Infrequent Access : Same performance as S3 Standard but up to 40% cheaper with 99.9% availability SLA and the same \u201c11 9s\u201d durability. S3 One Zone-Infrequent Access : A cheaper data tier in only one availability zone that can deliver an additional 25% savings over S3 Infrequent Access. It has the same durability, with 99.5% availability. S3 Reduced Redundancy Storage (RRS) : Previously this was a cheaper version of S3 providing 99.99% durability and 99.99% availability of objects. RRS cannot be used in a life cycling policy and is now more expensive than S3 Standard. S3 Glacier : Less than one-fifth the price of S3 Standard, designed for archiving and long-term storage. S3 Glacier Deep Archive : Costs four times less than Glacier and is the cheapest storage solution, at about $1 per terabyte per month. This solution is intended for very long-term storage. Data Life Cycling S3 Security \u00b6 There are three ways to grant access to an S3 bucket: IAM policy : You can attach IAM policies to users, groups, or roles to allow granular control over different levels of access (such as types of S3 API actions, like GET, PUT, or LIST) for one or more S3 buckets. Bucket policy : Attached to the bucket itself as an inline policy, a bucket policy can allow granular control over different levels of access (such as types of S3 API actions, like GET, PUT, or LIST) for the bucket itself. Bucket ACL : Attached to the bucket, an access control list (ACL) allows coarse-grained control over bucket access. ACLs are designed to easily share a bucket with a large group or anonymously when a need for read, write, or full control permissions over the bucket arises. To encrypt data at rest, you have three options in S3: S3 Server-Side Encryption (SSE-S3) : SSE-S3 provides built-in encryption with an AWS managed encryption key. This service is available on an S3 bucket at no additional cost. S3 SSE-KMS : SSE-KMS protects data by using a KMS-managed encryption key. This option gives you more control over the encryption keys to be used in S3, but the root encryption key of the KMS service is still AWS managed. S3 SSE-C : With the SSE-C option, S3 is configured with server-side encryption that uses a customer-provided encryption key. The encryption key is provided by the client within the request, and so each blob of data that is delivered to S3 is seamlessly encrypted with the customer-provided key. When the encryption is complete, S3 discards the encryption key so the only way to decrypt the data is to provide the same key when retrieving the object. DEPLOYING RELATIONAL DATABASES IN AWS \u00b6 Supported Database Types \u00b6 Currently the RDS service supports six different database engines that can be deployed from RDS: MySQL MariaDB PostgreSQL Amazon Aurora Oracle Microsoft SQL Serve Scaling Databases \u00b6 There are four general ways to scale database performance: Vertical scaling : You can give a single database engine more power by adding more CPU and RAM. Horizontal scaling : You can give a database cluster more power by adding more instances. Read offloading : You can add read replicas and redirect read traffic to them. Sharding : You can distribute the data across multiple database engines, with each one holding one section, or shard, of data. HANDLING NONRELATIONAL DATA IN AWS \u00b6 DynamoDB: A NoSQL key/value storage back end that is addressable via HTTP/HTTPS ElastiCache: An in-memory NoSQL storage back end DocumentDB: A NoSQL document storage back end Neptune: A NoSQL graphing solution for storing and addressing complex networked datasets Redshift: A columnar data warehousing solution that can scale to 2 PB per volume Redshift Spectrum: A serverless data warehousing solution that can address data sitting on S3 TimeStream: A time series recording solution for use with IoT and industrial telemetry Quantum Ledger: A ledger database designed for record streams, banking transactions, and so on Amazon DynamoDB \u00b6 The MySQL, MariaDB, and PostgreSQL engines all have similar general characteristics and support highly available Multi-AZ deployment topologies with a synchronous master/slave pair across two availability zones. All of them also have the ability to deploy multiple read replicas in the same region or in another region. Secondary Indexes \u00b6 Local secondary index (LSI) : The LSI can be considered an additional sort key for sifting through multiple entries of a certain primary key. This is very useful in applications where two ranges (the sort key and the secondary index) are required to retrieve the correct dataset. The LSI consumes some of the provisioned capacity of the table and can thus impact your performance calculations in case it is created. Global secondary index (GSI) : The GSI can be considered an additional primary key on which the data can be accessed. The GSI allows you to pivot a table and access the data through the key defined in the GSI and get a different view of the data. The GSI has its own provisioned read and write capacity units that can be set completely independently of the capacity units provisioned for the table Planning for DynamoDB Capacity \u00b6 When calculating capacities, you need to set both the read capacity units (RCUs) and write capacity units (WCUs): One RCU represents one strongly consistent 4 KB or two eventually consistent 4 KB reads. One WCU represents one write request of up to 1 KB in size. CACHING DATA IN AWS \u00b6 Amazon ElastiCache \u00b6 ElastiCache is a managed service that helps simplify the deployment of in-memory data stores in AWS.","title":"Chapter 4"},{"location":"aws-acd/chapter-4/#chapter-4-storing-data-in-aws","text":"","title":"Chapter 4 Storing Data in AWS"},{"location":"aws-acd/chapter-4/#storing-static-assets-in-aws","text":"","title":"STORING STATIC ASSETS IN AWS"},{"location":"aws-acd/chapter-4/#amazon-s3","text":"Two different ways of allowing access to S3 bucket Use an access control list (ACL) or a bucket ACL: quickly allow access to a large group of users, such as another account or everyone with a specific type of access to all the keys in the bucket. Use a bucket policy: A bucket policy can be used to granularly control access to a bucket and its contents.","title":"Amazon S3"},{"location":"aws-acd/chapter-4/#working-with-s3-in-the-aws-cli","text":"AWS create-bucket aws s3api create - bucket -- bucket bucket - name -- region region - id aws s3 cp / my - website / s3: //everyonelovesaws/ --recursive -- exclude \"*\" -- include \"*.html\" Access content within a bucket on S3 http { s }: // s3 . { region-id } . amazonaws . com / { bucket-name } / { optional key prefix } / { key-name } Bucket names are globally unique. Because every bucket name is essentially a subdomain of .s3.amazonaws.com , there is no way to make two buckets with the same name in all of AWS. Securing a static website through HTTPS with a free certificate attached to a CloudFront distribution. There are three status options: Disabled, Enabled, and Suspended. By default, a bucket has versioning disabled, but once it is enabled, it cannot be removed but only suspended.","title":"Working with S3 in the AWS CLI"},{"location":"aws-acd/chapter-4/#s3-storage-tiers","text":"s3 has six storage classes: S3 Standard : General-purpose online storage with 99.99% availability and 99.999999999% durability (that is, \u201c11 9s\u201d). S3 Infrequent Access : Same performance as S3 Standard but up to 40% cheaper with 99.9% availability SLA and the same \u201c11 9s\u201d durability. S3 One Zone-Infrequent Access : A cheaper data tier in only one availability zone that can deliver an additional 25% savings over S3 Infrequent Access. It has the same durability, with 99.5% availability. S3 Reduced Redundancy Storage (RRS) : Previously this was a cheaper version of S3 providing 99.99% durability and 99.99% availability of objects. RRS cannot be used in a life cycling policy and is now more expensive than S3 Standard. S3 Glacier : Less than one-fifth the price of S3 Standard, designed for archiving and long-term storage. S3 Glacier Deep Archive : Costs four times less than Glacier and is the cheapest storage solution, at about $1 per terabyte per month. This solution is intended for very long-term storage. Data Life Cycling","title":"S3 Storage Tiers"},{"location":"aws-acd/chapter-4/#s3-security","text":"There are three ways to grant access to an S3 bucket: IAM policy : You can attach IAM policies to users, groups, or roles to allow granular control over different levels of access (such as types of S3 API actions, like GET, PUT, or LIST) for one or more S3 buckets. Bucket policy : Attached to the bucket itself as an inline policy, a bucket policy can allow granular control over different levels of access (such as types of S3 API actions, like GET, PUT, or LIST) for the bucket itself. Bucket ACL : Attached to the bucket, an access control list (ACL) allows coarse-grained control over bucket access. ACLs are designed to easily share a bucket with a large group or anonymously when a need for read, write, or full control permissions over the bucket arises. To encrypt data at rest, you have three options in S3: S3 Server-Side Encryption (SSE-S3) : SSE-S3 provides built-in encryption with an AWS managed encryption key. This service is available on an S3 bucket at no additional cost. S3 SSE-KMS : SSE-KMS protects data by using a KMS-managed encryption key. This option gives you more control over the encryption keys to be used in S3, but the root encryption key of the KMS service is still AWS managed. S3 SSE-C : With the SSE-C option, S3 is configured with server-side encryption that uses a customer-provided encryption key. The encryption key is provided by the client within the request, and so each blob of data that is delivered to S3 is seamlessly encrypted with the customer-provided key. When the encryption is complete, S3 discards the encryption key so the only way to decrypt the data is to provide the same key when retrieving the object.","title":"S3 Security"},{"location":"aws-acd/chapter-4/#deploying-relational-databases-in-aws","text":"","title":"DEPLOYING RELATIONAL DATABASES IN AWS"},{"location":"aws-acd/chapter-4/#supported-database-types","text":"Currently the RDS service supports six different database engines that can be deployed from RDS: MySQL MariaDB PostgreSQL Amazon Aurora Oracle Microsoft SQL Serve","title":"Supported Database Types"},{"location":"aws-acd/chapter-4/#scaling-databases","text":"There are four general ways to scale database performance: Vertical scaling : You can give a single database engine more power by adding more CPU and RAM. Horizontal scaling : You can give a database cluster more power by adding more instances. Read offloading : You can add read replicas and redirect read traffic to them. Sharding : You can distribute the data across multiple database engines, with each one holding one section, or shard, of data.","title":"Scaling Databases"},{"location":"aws-acd/chapter-4/#handling-nonrelational-data-in-aws","text":"DynamoDB: A NoSQL key/value storage back end that is addressable via HTTP/HTTPS ElastiCache: An in-memory NoSQL storage back end DocumentDB: A NoSQL document storage back end Neptune: A NoSQL graphing solution for storing and addressing complex networked datasets Redshift: A columnar data warehousing solution that can scale to 2 PB per volume Redshift Spectrum: A serverless data warehousing solution that can address data sitting on S3 TimeStream: A time series recording solution for use with IoT and industrial telemetry Quantum Ledger: A ledger database designed for record streams, banking transactions, and so on","title":"HANDLING NONRELATIONAL DATA IN AWS"},{"location":"aws-acd/chapter-4/#amazon-dynamodb","text":"The MySQL, MariaDB, and PostgreSQL engines all have similar general characteristics and support highly available Multi-AZ deployment topologies with a synchronous master/slave pair across two availability zones. All of them also have the ability to deploy multiple read replicas in the same region or in another region.","title":"Amazon DynamoDB"},{"location":"aws-acd/chapter-4/#secondary-indexes","text":"Local secondary index (LSI) : The LSI can be considered an additional sort key for sifting through multiple entries of a certain primary key. This is very useful in applications where two ranges (the sort key and the secondary index) are required to retrieve the correct dataset. The LSI consumes some of the provisioned capacity of the table and can thus impact your performance calculations in case it is created. Global secondary index (GSI) : The GSI can be considered an additional primary key on which the data can be accessed. The GSI allows you to pivot a table and access the data through the key defined in the GSI and get a different view of the data. The GSI has its own provisioned read and write capacity units that can be set completely independently of the capacity units provisioned for the table","title":"Secondary Indexes"},{"location":"aws-acd/chapter-4/#planning-for-dynamodb-capacity","text":"When calculating capacities, you need to set both the read capacity units (RCUs) and write capacity units (WCUs): One RCU represents one strongly consistent 4 KB or two eventually consistent 4 KB reads. One WCU represents one write request of up to 1 KB in size.","title":"Planning for DynamoDB Capacity"},{"location":"aws-acd/chapter-4/#caching-data-in-aws","text":"","title":"CACHING DATA IN AWS"},{"location":"aws-acd/chapter-4/#amazon-elasticache","text":"ElastiCache is a managed service that helps simplify the deployment of in-memory data stores in AWS.","title":"Amazon ElastiCache"},{"location":"aws-acd/tutorials-point/","text":"AWS Certified Developer Associate Practice Exams 2021 \u00b6 AWS EC2 \u00b6 AWS EC2 Cheat Sheet AWS Lambda \u00b6 AWS Lambda Cheat Sheet Environment Variables, Stage Variables, Layers, Alias Environment variables for Lambda functions enable you to dynamically pass settings to your function code and libraries, without making changes to your code. Environment variables are key-value pairs that you create and modify as part of your function configuration, using either the AWS Lambda Console, the AWS Lambda CLI or the AWS Lambda SDK. Stage Variables is only available in API Gateway and not in Lambda. Layers is only used to pull in additional code and content in the form of layers. A layer is just a ZIP archive that contains libraries, a custom runtime, or other dependencies. Aliases is just like a pointer to a specific Lambda function version. By using aliases, you can access the Lambda function which the alias is pointing to, without the caller knowing the specific version the alias is pointing to. Concurrent executions refers to the number of executions of your function code that are happening at any given time. You can estimate the concurrent execution count, but the concurrent execution count will differ depending on whether or not your Lambda function is processing events from a poll-based event source. By default, AWS Lambda limits the total concurrent executions across all functions within a given region to 1000. This limit can be raised by requesting for AWS to increase the limit of the concurrent executions of your account. You can use this formula to estimate the capacity used by your function: concurrent executions = (invocations per second) x (average execution duration in seconds) For example, consider a Lambda function that processes Amazon S3 events. Suppose that the Lambda function takes on average three seconds and Amazon S3 publishes 10 events per second. Then, you will have 30 concurrent executions of your Lambda function. See the calculation shown below to visualize the process: (10 events per second) x (3 seconds average execution duration) = 30 concurrent executions In this scenario, it is expected that the Lambda function takes an average of 100 seconds for every execution with 50 requests per second. Using the formula above, the function will have 5,000 concurrent executions. (50 events per second) x (100 seconds average execution duration) = 5,000 concurrent executions In the Invoke API, you have 3 options to choose from for the InvocationType: RequestResponse (default) \u2013 Invoke the function synchronously . Keep the connection open until the function returns a response or times out. The API response includes the function response and additional data. ``` aws lambda invoke \u2013function-name my-function \u2013payload \u2018{ \u201ckey\u201d: \u201cvalue\u201d }\u2019 response.json { \u201cExecutedVersion\u201d: \u201c$LATEST\u201d, \u201cStatusCode\u201d: 200 } ``` Event \u2013 Invoke the function asynchronously . Send events that fail multiple times to the function\u2019s dead-letter queue (if it\u2019s configured). The API response only includes a status code. ``` aws lambda invoke \u2013function-name my-function \u2013invocation-type Event \u2013payload \u2018{ \u201ckey\u201d: \u201cvalue\u201d }\u2019 response.json { \u201cStatusCode\u201d: 202 } ``` DryRun \u2013 Validate parameter values and verify that the user or role has permission to invoke the function. For a Lambda function, you can have two types of integration: Lambda proxy integration: If your API does not require content encoding or caching, you only need to set the integration\u2019s HTTP method to POST, the integration endpoint URI to the ARN of the Lambda function invocation action of a specific Lambda function, and the credential to an IAM role with permissions to allow API Gateway to call the Lambda function on your behalf. Lambda custom integration: in addition to the proxy integration setup steps, you also specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response. Amazon CloudWatch \u00b6 Aamazon CloudWatch When you create an alarm, you specify three settings to enable CloudWatch to evaluate when to change the alarm state: Period is the length of time to evaluate the metric or expression to create each individual data point for an alarm. It is expressed in seconds. If you choose one minute as the period, there is one datapoint every minute. Evaluation Period is the number of the most recent periods, or data points, to evaluate when determining alarm state. Datapoints to Alarm is the number of data points within the evaluation period that must be breaching to cause the alarm to go to the ALARM state. The breaching data points do not have to be consecutive, they just must all be within the last number of data points equal to Evaluation Period. CloudWatch stores data about a metric as a series of data points. Each data point has an associated time stamp.Each metric is one of the following: Standard resolution , with data having a one-minute granularity High resolution , with data at a granularity of one second By default, your instance is enabled for basic monitoring. You can optionally enable detailed monitoring. After you enable detailed monitoring, the Amazon EC2 console displays monitoring graphs with a 1-minute period for the instance. The following table describes basic and detailed monitoring for instances. Basic : Data is available automatically in 5-minute periods at no charge. Detailed : Data is available in 1-minute periods for an additional cost. To get this level of data, you must specifically enable it for the instance. For the instances where you\u2019ve enabled detailed monitoring, you can also get aggregated data across groups of similar instances. AWS X-Ray \u00b6 AWS X-Ray Cheat Sheets AWS X-Ray, Amazon CloudWatch, Amazon Inspector, AWS CloudTrail AWS X-Ray You can use X-Ray to analyze both applications in development and in production, from simple three-tier applications to complex microservices applications consisting of thousands of services.AWS X-Ray works with Amazon EC2, Amazon EC2 Container Service (Amazon ECS), AWS Lambda, and AWS Elastic Beanstalk. You can use X-Ray with applications written in Java, Node.js, and .NET that are deployed on these services. Amazon CloudWatch is incorrect because although you can troubleshoot the issue by checking the logs, it is still better to use AWS X-Ray as it enables you to analyze and debug your serverless application more effectively. Amazon Inspector is incorrect because this is primarily used for EC2 and not for Lambda. AWS CloudTrail is incorrect because this will only enable you to track all API calls to your Lambda, DynamoDB, and SNS. It is still better to use AWS X-Ray to debug your application. Annotations, Metadata Annotations are simple key-value pairs that are indexed for use with filter expressions . Use annotations to record data that you want to use to group traces in the console, or when calling the GetTraceSummaries API. X-Ray indexes up to 50 annotations per trace. Metadata are key-value pairs with values of any type, including objects and lists, but that are not indexed. Use metadata to record data you want to store in the trace but don\u2019t need to use for searching traces. You can view annotations and metadata in the segment or subsegment details in the X-Ray console. Enable the X-Ray daemon by including the xray-daemon.config configuration file in the .ebextensions directory of your source code. AWS Lambda uses environment variables to facilitate communication with the X-Ray daemon and configure the X-Ray SDK. _X_AMZN_TRACE_ID : Contains the tracing header, which includes the sampling decision, trace ID, and parent segment ID. If Lambda receives a tracing header when your function is invoked, that header will be used to populate the _X_AMZN_TRACE_ID environment variable. If a tracing header was not received, Lambda will generate one for you. AWS_XRAY_CONTEXT_MISSING : The X-Ray SDK uses this variable to determine its behavior in the event that your function tries to record X-Ray data, but a tracing header is not available. Lambda sets this value to LOG_ERROR by default. AWS_XRAY_DAEMON_ADDRESS : This environment variable exposes the X-Ray daemon\u2019s address in the following format: IP_ADDRESS:PORT . You can use the X-Ray daemon\u2019s address to send trace data to the X-Ray daemon directly, without using the X-Ray SDK. To properly instrument your application hosted in an EC2 instance, you have to install the X-Ray daemon by using a user data script. This will install and run the daemon automatically when you launch the instance. To use the daemon on Amazon EC2, create a new instance profile role or add the managed policy to an existing one. This will grant the daemon permission to upload trace data to X-Ray. The AWS X-Ray daemon is a software application that listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS X-Ray API. The daemon works in conjunction with the AWS X-Ray SDKs and must be running so that data sent by the SDKs can reach the X-Ray service. Amazon API Gateway \u00b6 AWS API Gateway Cheat Sheets The metrics reported by API Gateway provide information that you can analyze in different ways. The list below shows some common uses for the metrics. These are suggestions to get you started, not a comprehensive list. Monitor the IntegrationLatency metrics to measure the responsiveness of the backend. Monitor the Latency metrics to measure the overall responsiveness of your API calls. Monitor the CacheHitCount and CacheMissCount metrics to optimize cache capacities to achieve a desired performance. A client of your API can invalidate an existing cache entry and reload it from the integration endpoint for individual requests. The client must send a request that contains the Cache-Control: max-age=0 header.Ticking the Require authorization checkbox ensures that not every client can invalidate the API cache. If most or all of the clients invalidate the API cache, this could significantly increase the latency of your API. You can integrate an API method in your API Gateway with a custom HTTP endpoint of your application in two ways: HTTP proxy integration \u2014 HTTP_PROXY HTTP custom integration \u2014 HTTP Programmatically, you choose an integration type by setting the type property on the Integration resource. For the Lambda proxy integration, the value is AWS_PROXY . For the Lambda custom integration and all other AWS integrations, it is AWS. For the HTTP proxy integration and HTTP integration, the value is HTTP_PROXY and HTTP , respectively. For the mock integration, the type value is MOCK . For the integration timeout, the range is from 50 milliseconds to 29 seconds for all integration types, including Lambda, Lambda proxy, HTTP, HTTP proxy, and AWS integrations.The following are the Gateway response types which are associated with the HTTP 504 error in API Gateway: INTEGRATION_FAILURE \u2013 The gateway response for an integration failed error. If the response type is unspecified, this response defaults to the DEFAULT_5XX type. INTEGRATION_TIMEOUT \u2013 The gateway response for an integration timed out error. If the response type is unspecified, this response defaults to the DEFAULT_5XX type. Amazon API Gateway Lambda proxy integration is a simple, powerful, and nimble mechanism to build an API with a setup of a single API method. there is an incompatible output returned from a Lambda proxy integration backend. : the Lambda function returns the result in XML format, it will cause the 502 errors in the API Gateway. There has been an occasional out-of-order invocation due to heavy loads :because although this is a valid cause of a 502 error, the issue is most likely caused by the Lambda function\u2019s XML response instead of JSON. The endpoint request timed-out : this will likely result in 504 errors. Amazon VPC \u00b6 Amazon VPC Cheat Sheets VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be published to Amazon CloudWatch Logs and Amazon S3. A route table contains a set of rules, called routes, that are used to determine where network traffic is directed. Each subnet in your VPC must be associated with a route table; the table controls the routing for the subnet. A subnet can only be associated with one route table at a time, but you can associate multiple subnets with the same route table. Thus, the correct answer is a subnet can only be associated with one route table at a time. Amazon CloudFront \u00b6 Amazon CloudFront Cheat Sheet CloudFront HTTP/S You can configure one or more cache behaviors in your CloudFront distribution to require HTTPS for communication between viewers and CloudFront. You also can configure one or more cache behaviors to allow both HTTP and HTTPS, so that CloudFront requires HTTPS for some objects but not for others.To implement this setup, you have to change the Origin Protocol Policy setting for the applicable origins in your distribution. If you\u2019re using the domain name that CloudFront assigned to your distribution, such as dtut0rial5d0j0.cloudfront.net, you change the Viewer Protocol Policy setting for one or more cache behaviors to require HTTPS communication. With this configuration, CloudFront provides the SSL/TLS certificate. Amazon S3 \u00b6 Amazon S3 Cheat Sheet Server-side encryption protects data at rest. If you use Server-Side Encryption with Amazon S3-Managed Encryption Keys (SSE-S3), Amazon S3 will encrypt each object with a unique key and as an additional safeguard, it encrypts the key itself with a master key that it rotates regularly. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data. x-amz-server-side-encryption header to request server-side encryption. server-side encryption with customer-provided encryption keys (SSE-C), you must provide encryption key information using the following request headers: x-amz-server-side\u200b-encryption\u200b-customer-algorithm x-amz-server-side\u200b-encryption\u200b-customer-key x-amz-server-side\u200b-encryption\u200b-customer-key-MD5 since the put-bucket-policy command can only be used to apply policy at the bucket level, not on objects. You can use S3 Access Control Lists (ACLs) instead to manage permissions of S3 objects. To enable the CRR(cross-region replication) feature in S3, the following items should be met: The source and destination buckets must have versioning enabled. The source and destination buckets must be in different AWS Regions. Amazon S3 must have permissions to replicate objects from that source bucket to the destination bucket on your behalf. AWS CodeCommit \u00b6 AWS CodeCommit Cheat Sheet In CodeCommit, the primary resource is a repository. Each resource has a unique Amazon Resource Names (ARN) associated with it. In a policy, you use an Amazon Resource Name (ARN) to identify the resource that the policy applies to. To manage access to AWS resources, you attach permissions policies to IAM identities. You use identity-based policies to control access to repositories. The codecommit:CreateRepository and codecommit:DeleteRepository permissions enable you to create and delete a CodeCommit repository respectively. Hence, these two permissions are the correct permissions that should be given to the technical manager. codecommit:GitPull and codecommit:GitPush permissions are incorrect because the codecommit:GitPull permission simply pulls information from a CodeCommit repository to a local repo and the codecommit:GitPush permission is just used to push information from a local repo to a CodeCommit repository. codecommit:CreateBranch and codecommit:DeleteBranch permissions are incorrect because these enable you to create and delete a specific branch and not a repository. For this scenario, you have to use the codecommit:CreateRepository and codecommit:DeleteRepository permissions instead. codecommit:* is incorrect because although this permission will allow all actions to the code repository. This entails a security risk and violates the standard security practice of granting only the permissions required to perform certain tasks. AWS CodeDeploy \u00b6 AWS CodeDeploy Cheat Sheet CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services. Canary deployment configuration, the traffic is shifted in two increments. You can choose from predefined canary options that specify the percentage of traffic shifted to your updated Lambda function version in the first increment and the interval, in minutes, before the remaining traffic is shifted in the second increment. Hence, this is the correct answer which will satisfy the requirement for the given scenario. Linear will cause the traffic to be shifted in equal increments with an equal number of minutes between each increment. You can choose from predefined linear options that specify the percentage of traffic shifted in each increment and the number of minutes between each increment. All-at-once with this deployment configuration, the traffic is shifted from the original Lambda function to the updated Lambda function version all at once. Rolling with additional batch this is only applicable in Elastic Beanstalk and not for Lambda. CodeDeploy provides two deployment type options: In-place deployment: The application on each instance in the deployment group is stopped, the latest application revision is installed, and the new version of the application is started and validated. You can use a load balancer so that each instance is deregistered during its deployment and then restored to service after the deployment is complete. Only deployments that use the EC2/On-Premises compute platform can use in-place deployments. AWS Lambda compute platform deployments cannot use an in-place deployment type. Blue/green deployment: The behavior of your deployment depends on which compute platform you use: \u2013 Blue/green on an EC2/On-Premises compute platform: The instances in a deployment group (the original environment) are replaced by a different set of instances (the replacement environment). If you use an EC2/On-Premises compute platform, be aware that blue/green deployments work with Amazon EC2 instances only. \u2013 Blue/green on an AWS Lambda compute platform: Traffic is shifted from your current serverless environment to one with your updated Lambda function versions. You can specify Lambda functions that perform validation tests and choose the way in which the traffic shift occurs. All AWS Lambda compute platform deployments are blue/green deployments. For this reason, you do not need to specify a deployment type. \u2013 Blue/green on an Amazon ECS compute platform: Traffic is shifted from the task set with the original version of a containerized application in an Amazon ECS service to a replacement task set in the same service. The protocol and port of a specified load balancer listener are used to reroute production traffic. During deployment, a test listener can be used to serve traffic to the replacement task set while validation tests are run. The CodeDeploy agent is a software package that, when installed and configured on an instance, makes it possible for that instance to be used in CodeDeploy deployments. The CodeDeploy agent communicates outbound using HTTPS over port 443. CodeDeploy agent is required only if you deploy to an EC2/On-Premises compute platform. The agent is not required for deployments that use the Amazon ECS or AWS Lambda compute platform. Amazon DynamoDB \u00b6 Amazon DynamoDB Cheat Sheet In general, Scan operations are less efficient than other operations in DynamoDB. A Scan operation always scans the entire table or secondary index. If possible, you should avoid using a Scan operation on a large table or index with a filter that removes many results. Also, as a table or index grows, the Scan operation slows. The Scan operation examines every item for the requested values and can use up the provisioned throughput for a large table or index in a single operation. For faster response times, design your tables and indexes so that your applications can use Query instead of Scan . For tables, you can also consider using the GetItem and BatchGetItem APIs. Time To Live (TTL) for DynamoDB allows you to define when items in a table expire so that they can be automatically deleted from the database.TTL is provided at no extra cost as a way to reduce storage usage and reduce the cost of storing irrelevant data without using provisioned throughput. With TTL enabled on a table, you can set a timestamp for deletion on a per-item basis, allowing you to limit storage usage to only those records that are relevant. DynamoDB Streams enables solutions such as these, and many others. DynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table, and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near real time.A DynamoDB stream is an ordered flow of information about changes to items in an Amazon DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. Amazon DynamoDB is also integrated with AWS Lambda so that you can create triggers\u2014pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables.When an item in the table is modified, StreamViewType determines what information are written to the stream for this table. Valid values for StreamViewType are KEYS_ONLY , NEW_IMAGE , OLD_IMAGE , and NEW_AND_OLD_IMAGES . OLD_IMAGE type, the entire item which has the previous value as it appeared before it was modified is written to the stream. KEYS_ONLY type, it will only write the key attributes of the modified item to the stream. NEW_IMAGE type, it will configure the stream to write the entire item with its new value as it appears after it was modified. NEW_AND_OLD_IMAGES type, it will send both the new and the old item images of the item to the stream Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement \u2013 from milliseconds to microseconds DynamoDB supports two types of secondary indexes: Global secondary index \u2014 an index with a partition key and a sort key that can be different from those on the base table. A global secondary index is considered \u201cglobal\u201d because queries on the index can span all of the data in the base table, across all partitions. Local secondary index \u2014 an index that has the same partition key as the base table, but a different sort key. A local secondary index is \u201clocal\u201d in the sense that every partition of a local secondary index is scoped to a base table partition that has the same partition key value. atomic counter \u2014 a numeric attribute that is incremented, unconditionally, without interfering with other write requests. (All write requests are applied in the order in which they were received). With an atomic counter, the updates are not idempotent. In other words, the numeric value will increment each time you call UpdateItem. conditional update - where overcounting or undercounting cannot be tolerated (For example, in a banking application). In this case, it is safer to use a conditional update instead of an atomic counter. To return the number of write capacity units consumed by any of these operations, set the ReturnConsumedCapacity parameter to one of the following: TOTAL \u2014 returns the total number of write capacity units consumed. INDEXES \u2014 returns the total number of write capacity units consumed, with subtotals for the table and any secondary indexes that were affected by the operation. NONE \u2014 no write capacity details are returned. (This is the default.) Local Secondary Index((LSI) - You cannot add a local secondary index to an existing table, nor can you delete any local secondary indexes that currently exist. Global Secondary Index (GSI) - You could add global secondary index to an existing table. Local secondary indexes are created at the same time that you create a table. You cannot add a local secondary index to an existing table, nor can you delete any local secondary indexes that currently exist. Global Secondary Index vs. Local Secondary Index AWS Elastic Beanstalk \u00b6 AWS Elastic Beanstalk Cheat Sheets In ElasticBeanstalk, you can choose from a variety of deployment methods: All at once \u2013 Deploy the new version to all instances simultaneously. All instances in your environment are out of service for a short time while the deployment occurs. Rolling \u2013 Deploy the new version in batches. Each batch is taken out of service during the deployment phase, reducing your environment\u2019s capacity by the number of instances in a batch. Rolling with additional batch \u2013 Deploy the new version in batches, but first launch a new batch of instances to ensure full capacity during the deployment process. Immutable \u2013 Deploy the new version to a fresh group of instances by performing an immutable update. Blue/Green \u2013 Deploy the new version to a separate environment, and then swap CNAMEs of the two environments to redirect traffic to the new version instantly. In Elastic Beanstalk, env.yaml : you can include a YAML formatted environment manifest in the root of your application source bundle to configure the environment name, solution stack and environment links to use when creating your environment. An environment manifest uses the same format as Saved Configurations. Dockerrun.aws.json :this configuration file is primarily used in multicontainer Docker environments that are hosted in Elastic Beanstalk. This can be used alone or combined with source code and content in a source bundle to create an environment on a Docker platform. env.config : this is just a custom configuration file which is not readily available in Elastic Beanstalk. Configuration files are YAML- or JSON-formatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle. The more appropriate configuration file to use here is the env.yaml which can help you configure the environment name, solution stack, and environment links to use when creating your environment. cron.yaml : this configuration file is primarily used to define periodic tasks that add jobs to your worker environment\u2019s queue automatically at a regular interval. Amazon ECS \u00b6 Amazon ECS Cheat Sheet A task placement strategy is an algorithm for selecting instances for task placement or tasks for termination. Task placement strategies can be specified when either running a task or creating a new service.Amazon ECS supports the following task placement strategies: binpack \u2013 Place tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use. random \u2013 Place tasks randomly. spread \u2013 Place tasks evenly based on the specified value. Accepted values are attribute key-value pairs, instanceId, or host. The Random task placement strategy is fairly straightforward as it doesn\u2019t require further parameters. The two other strategies, such as binpack and spread , take opposite actions. Binpack places tasks on as few instances as possible, helping to optimize resource utilization, while spread places tasks evenly across your cluster to help maximize availability. By default, ECS uses spread with the ecs.availability-zone attribute to place tasks. Random places tasks on instances at random yet still honors the other constraints that you specified, implicitly or explicitly. Specifically, it still makes sure that tasks are scheduled on instances with enough resources to run them. Port mappings allow containers to access ports on the host container instance to send or receive traffic. Port mappings are specified as part of the container definition which can be configured in the task definition.For task definitions that use the awsvpc network mode, you should only specify the containerPort . The hostPort can be left blank or it must be the same value as the containerPort . AWS (Serverless Application Model) SAM \u00b6 A serverless application can include one or more nested applications. AWS::Serverless::Application \u2013 define a nested application in your serverless application. AWS::Serverless::Function \u2013 describes configuration information for creating a Lambda function. AWS::Serverless::LayerVersion \u2013 creates a Lambda layer version (LayerVersion) AWS::Serverless::Api \u2013 describes an API Gateway resource Amazon Elasticache \u00b6 Amazon Elasticache Cheat Sheet Amazon ElastiCache is an in-memory key/value store that sits between your application and the data store (database) that it accesses. Whenever your application requests data, it first makes the request to the ElastiCache cache. Lazy Loading as its name implies, is a caching strategy that loads data into the cache only when necessary. Write Through This strategy adds data or updates data in the cache whenever data are written to the database, and not on the event of a cache miss. Adding TTL just adds a time to live (TTL) value to each write operation, which will eventually expire the cache object. Amazon KMS \u00b6 The GenerateDataKeyWithoutPlaintext API generates a unique data key. This operation returns a data key that is encrypted under a customer master key (CMK) that you specify. GenerateDataKeyWithoutPlaintext is identical to GenerateDataKey except that it returns only the encrypted copy of the data key. GenerateDataKey this operation also returns a plaintext copy of the data key along with the copy of the encrypted data key under a customer master key (CMK) that you specified. Take note that the scenario explicitly mentioned that the API must return only the encrypted copy of the data key which will be used later for encryption. Although this API can be used in this scenario, it is not recommended since the actual encryption process of the data happens at a later time and not in real-time. Amazon SQS \u00b6 Amazon SQS Cheat Sheet Amazon SQS long polling is a way to retrieve messages from your Amazon SQS queues. While the regular short polling returns immediately, even if the message queue being polled is empty, long polling doesn\u2019t return a response until a message arrives in the message queue, or the long poll times out.Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren\u2019t included in a response). This type of polling is suitable if the new messages that are being added to the SQS queue arrive less frequently. Amazon SNS \u00b6 Amazon SNS Cheat Sheet Amazon Simple Notification Service (Amazon SNS) is a web service that coordinates and manages the delivery or sending of messages to subscribing endpoints or clients.With Amazon SNS, you have the ability to send push notification messages directly to apps on mobile devices. Push notification messages sent to a mobile endpoint can appear in the mobile app as message alerts, badge updates, or even sound alerts. SQS this service is mainly used for message queues and not for notifications. SES this service is mainly used for sending emails and not for notifications. Amazon Kinesis \u00b6 Amazon Kinesis Cheat Sheet when you use the Kinesis Client Library (KCL), you should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. Amazon Kinesis Data Streams supports changes to the data record retention period of your stream. A Kinesis data stream is an ordered sequence of data records meant to be written to and read from in real time. Data records are therefore stored in shards in your stream temporarily. The time period from when a record is added to when it is no longer accessible is called the retention period. A Kinesis data stream stores records from 24 hours by default, up to 168 hours. AWS Systems Manager \u00b6 AWS Systems Manager Cheat Sheet AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. You can store values as plain text or encrypted data. You can then reference values by using the unique name that you specified when you created the parameter. AWS Secrets Manager \u00b6 AWS Secrets Manager Cheat Sheet you can configure Secrets Manager to automatically rotate the secret for you according to a schedule that you specify. AWS IAM \u00b6 AWS IAM Cheat Sheet Below is the summary of the available STS API: AssumeRole \u2013 is useful for allowing existing IAM users to access AWS resources that they don\u2019t already have access to. For example, the user might need access to resources in another AWS account. It is also useful as a means to temporarily gain privileged access\u2014for example, to provide multi-factor authentication (MFA). You must call this API using existing IAM user credentials. AssumeRoleWithWebIdentity \u2013 returns a set of temporary security credentials for federated users who are authenticated through a public identity provider. Examples of public identity providers include Login with Amazon, Facebook, Google, or any OpenID Connect (OIDC)-compatible identity provider. AssumeRoleWithSAML \u2013 returns a set of temporary security credentials for federated users who are authenticated by your organization\u2019s existing identity system. The users must also use SAML 2.0 (Security Assertion Markup Language) to pass authentication and authorization information to AWS. This API operation is useful in organizations that have integrated their identity systems (such as Windows Active Directory or OpenLDAP) with software that can produce SAML assertions. GetFederationToken \u2013 returns a set of temporary security credentials (consisting of an access key ID, a secret access key, and a security token) for a federated user. A typical use is in a proxy application that gets temporary security credentials on behalf of distributed applications inside a corporate network. You must call the GetFederationToken operation using the long-term security credentials of an IAM user. GetSessionToken \u2013 returns a set of temporary security credentials to an existing IAM user. This is useful for providing enhanced security, such as allowing AWS requests only when MFA is enabled for the IAM user. Because the credentials are temporary, they provide enhanced security when you have an IAM user who accesses your resources through a less secure environment. Amazon QuickSight \u00b6 Amazon QuickSight Cheat Sheet Amazon QuickSight is a fast, cloud-powered business intelligence service that makes it easy to deliver insights to everyone in your organization. As a fully managed service, QuickSight lets you easily create and publish interactive dashboards that include ML Insights. Dashboards can then be accessed from any device and embedded into your applications, portals, and websites. Amazon S3 Glacier \u00b6 Amazon S3 Glacier Cheat Sheet There are three options for retrieving data with varying access times and cost: Standard retrievals allow you to access any of your archives within several hours. Standard retrievals typically complete within 3\u20135 hours. This is the default option. Bulk retrievals are Glacier\u2019s lowest-cost retrieval option, which you can use to retrieve large amounts, even petabytes, of data inexpensively in a day. Bulk retrievals typically complete within 5\u201312 hours. Expedited retrievals allow you to quickly access your data when occasional urgent requests for a subset of archives are required. Expedited retrievals are typically made available within 1\u20135 minutes AWS Elastic Load Balancing \u00b6 AWS Elastic Load Balancing Cheat Sheet Application Load Balancers provide two advanced options that you may want to configure when you use ALBs with AWS Lambda: support for multi-value headers health check configurations. AWS Step Functions \u00b6 AWS Step Functions Cheat Sheet States can perform a variety of functions in your state machine: Task State \u2013 Do some work in your state machine Choice State \u2013 Make a choice between branches of execution Fail or Succeed State \u2013 Stop execution with failure or success Pass State \u2013 Simply pass its input to its output or inject some fixed data, without performing work. Wait State \u2013 Provide a delay for a certain amount of time or until a specified time/date. Parallel State \u2013 Begin parallel branches of execution. Map State \u2013 Dynamically iterate steps. AWS S3 Glacier \u00b6 Amazon S3 Glacier Cheat Sheet There are three options for retrieving data with varying access times and cost: Standard retrievals allow you to access any of your archives within several hours. Standard retrievals typically complete within 3\u20135 hours. This is the default option. Bulk retrievals are Glacier\u2019s lowest-cost retrieval option, which you can use to retrieve large amounts, even petabytes, of data inexpensively in a day. Bulk retrievals typically complete within 5\u201312 hours. Expedited retrievals allow you to quickly access your data when occasional urgent requests for a subset of archives are required. Expedited retrievals are typically made available within 1\u20135 minutes. Comparison of AWS Services \u00b6 Comparison of AWS Services Cheat Sheets Redis can provide a much more durable and powerful cache layer to the prototype distributed system, however, you should take note of one keyword in the requirement: multithreaded . In terms of commands execution, Redis is mostly a single-threaded server . It is not designed to benefit from multiple CPU cores unlike Memcached, however, you can launch several Redis instances to scale out on several cores if needed. Memcached is a more suitable choice since the scenario specifies that the system will run large nodes with multiple cores or threads which Memcached can adequately provide. You can choose Memcached over Redis if you have the following requirements: You need the simplest model possible. You need to run large nodes with multiple cores or threads. You need the ability to scale out and in, adding and removing nodes as demand on your system increases and decreases. You need to cache objects, such as a database. The two main components of Amazon Cognito are user pools and identity pools. User pools are user directories that provide sign-up and sign-in options for your app users. Identity pools enable you to grant your users access to other AWS services. You can use identity pools and user pools separately or together. Amazon Cognito identity pools provide temporary AWS credentials for users who are guests (unauthenticated) and for users who have been authenticated and received a token. An identity pool is a store of user identity data specific to your account.Amazon Cognito identity pools enable you to create unique identities and assign permissions for users. Your identity pool can include: Users in an Amazon Cognito user pool Users who authenticate with external identity providers such as Facebook, Google, or a SAML-based identity provider Users authenticated via your own existing authentication process Databases employ locking mechanisms to ensure that data is always updated to the latest version and is concurrent. There are multiple types of locking strategies that benefit different use cases. Some of these are: Optimistic Locking: Optimistic locking is a strategy to ensure that the client-side item that you are updating (or deleting) is the same as the item in DynamoDB. If you use this strategy, then your database writes are protected from being overwritten by the writes of others \u2014 and vice-verse. Pessimistic Locking Overly Optimistic Locking Other Important Link \u00b6 Amazon Simple Workflow (SWF) vs AWS Step Functions vs Amazon SQS Comparison of AWS Services Cheat Sheets Kinesis Scaling, Resharding and Parallel Processing Redis (cluster mode enabled vs disabled) vs Memcached AWS Lambda Integration with Amazon DynamoDB Streams RDS vs DynamoDB DynamoDB Scan vs Query Elastic Beanstalk vs CloudFormation vs OpsWorks vs CodeDeploy Redis vs Memcached \u00b6 You can choose Memcached over Redis if you have the following requirements: You need the simplest model possible. You need to run large nodes with multiple cores or threads. You need the ability to scale out and in, adding and removing nodes as demand on your system increases and decreases. You need to cache objects, such as a database. Amazon Kinesis Data Firehose \u00b6 Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you\u2019re already using today. AWS Step Functions \u00b6 AWS Step Functions is a web service that enables you to coordinate the components of distributed applications and microservices using visual workflows. You build applications from individual components that each perform a discrete function, or task, allowing you to scale and change applications quickly. The following are key features of AWS Step Functions: Step Functions is based on the concepts of tasks and state machines. You define state machines using the JSON-based Amazon States Language. The Step Functions console displays a graphical view of your state machine\u2019s structure. This provides a way to visually check your state machine\u2019s logic and monitor executions.","title":"Tutorials Test"},{"location":"aws-acd/tutorials-point/#aws-certified-developer-associate-practice-exams-2021","text":"","title":"AWS Certified Developer Associate Practice Exams 2021"},{"location":"aws-acd/tutorials-point/#aws-ec2","text":"AWS EC2 Cheat Sheet","title":"AWS EC2"},{"location":"aws-acd/tutorials-point/#aws-lambda","text":"AWS Lambda Cheat Sheet Environment Variables, Stage Variables, Layers, Alias Environment variables for Lambda functions enable you to dynamically pass settings to your function code and libraries, without making changes to your code. Environment variables are key-value pairs that you create and modify as part of your function configuration, using either the AWS Lambda Console, the AWS Lambda CLI or the AWS Lambda SDK. Stage Variables is only available in API Gateway and not in Lambda. Layers is only used to pull in additional code and content in the form of layers. A layer is just a ZIP archive that contains libraries, a custom runtime, or other dependencies. Aliases is just like a pointer to a specific Lambda function version. By using aliases, you can access the Lambda function which the alias is pointing to, without the caller knowing the specific version the alias is pointing to. Concurrent executions refers to the number of executions of your function code that are happening at any given time. You can estimate the concurrent execution count, but the concurrent execution count will differ depending on whether or not your Lambda function is processing events from a poll-based event source. By default, AWS Lambda limits the total concurrent executions across all functions within a given region to 1000. This limit can be raised by requesting for AWS to increase the limit of the concurrent executions of your account. You can use this formula to estimate the capacity used by your function: concurrent executions = (invocations per second) x (average execution duration in seconds) For example, consider a Lambda function that processes Amazon S3 events. Suppose that the Lambda function takes on average three seconds and Amazon S3 publishes 10 events per second. Then, you will have 30 concurrent executions of your Lambda function. See the calculation shown below to visualize the process: (10 events per second) x (3 seconds average execution duration) = 30 concurrent executions In this scenario, it is expected that the Lambda function takes an average of 100 seconds for every execution with 50 requests per second. Using the formula above, the function will have 5,000 concurrent executions. (50 events per second) x (100 seconds average execution duration) = 5,000 concurrent executions In the Invoke API, you have 3 options to choose from for the InvocationType: RequestResponse (default) \u2013 Invoke the function synchronously . Keep the connection open until the function returns a response or times out. The API response includes the function response and additional data. ``` aws lambda invoke \u2013function-name my-function \u2013payload \u2018{ \u201ckey\u201d: \u201cvalue\u201d }\u2019 response.json { \u201cExecutedVersion\u201d: \u201c$LATEST\u201d, \u201cStatusCode\u201d: 200 } ``` Event \u2013 Invoke the function asynchronously . Send events that fail multiple times to the function\u2019s dead-letter queue (if it\u2019s configured). The API response only includes a status code. ``` aws lambda invoke \u2013function-name my-function \u2013invocation-type Event \u2013payload \u2018{ \u201ckey\u201d: \u201cvalue\u201d }\u2019 response.json { \u201cStatusCode\u201d: 202 } ``` DryRun \u2013 Validate parameter values and verify that the user or role has permission to invoke the function. For a Lambda function, you can have two types of integration: Lambda proxy integration: If your API does not require content encoding or caching, you only need to set the integration\u2019s HTTP method to POST, the integration endpoint URI to the ARN of the Lambda function invocation action of a specific Lambda function, and the credential to an IAM role with permissions to allow API Gateway to call the Lambda function on your behalf. Lambda custom integration: in addition to the proxy integration setup steps, you also specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response.","title":"AWS Lambda"},{"location":"aws-acd/tutorials-point/#amazon-cloudwatch","text":"Aamazon CloudWatch When you create an alarm, you specify three settings to enable CloudWatch to evaluate when to change the alarm state: Period is the length of time to evaluate the metric or expression to create each individual data point for an alarm. It is expressed in seconds. If you choose one minute as the period, there is one datapoint every minute. Evaluation Period is the number of the most recent periods, or data points, to evaluate when determining alarm state. Datapoints to Alarm is the number of data points within the evaluation period that must be breaching to cause the alarm to go to the ALARM state. The breaching data points do not have to be consecutive, they just must all be within the last number of data points equal to Evaluation Period. CloudWatch stores data about a metric as a series of data points. Each data point has an associated time stamp.Each metric is one of the following: Standard resolution , with data having a one-minute granularity High resolution , with data at a granularity of one second By default, your instance is enabled for basic monitoring. You can optionally enable detailed monitoring. After you enable detailed monitoring, the Amazon EC2 console displays monitoring graphs with a 1-minute period for the instance. The following table describes basic and detailed monitoring for instances. Basic : Data is available automatically in 5-minute periods at no charge. Detailed : Data is available in 1-minute periods for an additional cost. To get this level of data, you must specifically enable it for the instance. For the instances where you\u2019ve enabled detailed monitoring, you can also get aggregated data across groups of similar instances.","title":"Amazon CloudWatch"},{"location":"aws-acd/tutorials-point/#aws-x-ray","text":"AWS X-Ray Cheat Sheets AWS X-Ray, Amazon CloudWatch, Amazon Inspector, AWS CloudTrail AWS X-Ray You can use X-Ray to analyze both applications in development and in production, from simple three-tier applications to complex microservices applications consisting of thousands of services.AWS X-Ray works with Amazon EC2, Amazon EC2 Container Service (Amazon ECS), AWS Lambda, and AWS Elastic Beanstalk. You can use X-Ray with applications written in Java, Node.js, and .NET that are deployed on these services. Amazon CloudWatch is incorrect because although you can troubleshoot the issue by checking the logs, it is still better to use AWS X-Ray as it enables you to analyze and debug your serverless application more effectively. Amazon Inspector is incorrect because this is primarily used for EC2 and not for Lambda. AWS CloudTrail is incorrect because this will only enable you to track all API calls to your Lambda, DynamoDB, and SNS. It is still better to use AWS X-Ray to debug your application. Annotations, Metadata Annotations are simple key-value pairs that are indexed for use with filter expressions . Use annotations to record data that you want to use to group traces in the console, or when calling the GetTraceSummaries API. X-Ray indexes up to 50 annotations per trace. Metadata are key-value pairs with values of any type, including objects and lists, but that are not indexed. Use metadata to record data you want to store in the trace but don\u2019t need to use for searching traces. You can view annotations and metadata in the segment or subsegment details in the X-Ray console. Enable the X-Ray daemon by including the xray-daemon.config configuration file in the .ebextensions directory of your source code. AWS Lambda uses environment variables to facilitate communication with the X-Ray daemon and configure the X-Ray SDK. _X_AMZN_TRACE_ID : Contains the tracing header, which includes the sampling decision, trace ID, and parent segment ID. If Lambda receives a tracing header when your function is invoked, that header will be used to populate the _X_AMZN_TRACE_ID environment variable. If a tracing header was not received, Lambda will generate one for you. AWS_XRAY_CONTEXT_MISSING : The X-Ray SDK uses this variable to determine its behavior in the event that your function tries to record X-Ray data, but a tracing header is not available. Lambda sets this value to LOG_ERROR by default. AWS_XRAY_DAEMON_ADDRESS : This environment variable exposes the X-Ray daemon\u2019s address in the following format: IP_ADDRESS:PORT . You can use the X-Ray daemon\u2019s address to send trace data to the X-Ray daemon directly, without using the X-Ray SDK. To properly instrument your application hosted in an EC2 instance, you have to install the X-Ray daemon by using a user data script. This will install and run the daemon automatically when you launch the instance. To use the daemon on Amazon EC2, create a new instance profile role or add the managed policy to an existing one. This will grant the daemon permission to upload trace data to X-Ray. The AWS X-Ray daemon is a software application that listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS X-Ray API. The daemon works in conjunction with the AWS X-Ray SDKs and must be running so that data sent by the SDKs can reach the X-Ray service.","title":"AWS X-Ray"},{"location":"aws-acd/tutorials-point/#amazon-api-gateway","text":"AWS API Gateway Cheat Sheets The metrics reported by API Gateway provide information that you can analyze in different ways. The list below shows some common uses for the metrics. These are suggestions to get you started, not a comprehensive list. Monitor the IntegrationLatency metrics to measure the responsiveness of the backend. Monitor the Latency metrics to measure the overall responsiveness of your API calls. Monitor the CacheHitCount and CacheMissCount metrics to optimize cache capacities to achieve a desired performance. A client of your API can invalidate an existing cache entry and reload it from the integration endpoint for individual requests. The client must send a request that contains the Cache-Control: max-age=0 header.Ticking the Require authorization checkbox ensures that not every client can invalidate the API cache. If most or all of the clients invalidate the API cache, this could significantly increase the latency of your API. You can integrate an API method in your API Gateway with a custom HTTP endpoint of your application in two ways: HTTP proxy integration \u2014 HTTP_PROXY HTTP custom integration \u2014 HTTP Programmatically, you choose an integration type by setting the type property on the Integration resource. For the Lambda proxy integration, the value is AWS_PROXY . For the Lambda custom integration and all other AWS integrations, it is AWS. For the HTTP proxy integration and HTTP integration, the value is HTTP_PROXY and HTTP , respectively. For the mock integration, the type value is MOCK . For the integration timeout, the range is from 50 milliseconds to 29 seconds for all integration types, including Lambda, Lambda proxy, HTTP, HTTP proxy, and AWS integrations.The following are the Gateway response types which are associated with the HTTP 504 error in API Gateway: INTEGRATION_FAILURE \u2013 The gateway response for an integration failed error. If the response type is unspecified, this response defaults to the DEFAULT_5XX type. INTEGRATION_TIMEOUT \u2013 The gateway response for an integration timed out error. If the response type is unspecified, this response defaults to the DEFAULT_5XX type. Amazon API Gateway Lambda proxy integration is a simple, powerful, and nimble mechanism to build an API with a setup of a single API method. there is an incompatible output returned from a Lambda proxy integration backend. : the Lambda function returns the result in XML format, it will cause the 502 errors in the API Gateway. There has been an occasional out-of-order invocation due to heavy loads :because although this is a valid cause of a 502 error, the issue is most likely caused by the Lambda function\u2019s XML response instead of JSON. The endpoint request timed-out : this will likely result in 504 errors.","title":"Amazon API Gateway"},{"location":"aws-acd/tutorials-point/#amazon-vpc","text":"Amazon VPC Cheat Sheets VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be published to Amazon CloudWatch Logs and Amazon S3. A route table contains a set of rules, called routes, that are used to determine where network traffic is directed. Each subnet in your VPC must be associated with a route table; the table controls the routing for the subnet. A subnet can only be associated with one route table at a time, but you can associate multiple subnets with the same route table. Thus, the correct answer is a subnet can only be associated with one route table at a time.","title":"Amazon VPC"},{"location":"aws-acd/tutorials-point/#amazon-cloudfront","text":"Amazon CloudFront Cheat Sheet CloudFront HTTP/S You can configure one or more cache behaviors in your CloudFront distribution to require HTTPS for communication between viewers and CloudFront. You also can configure one or more cache behaviors to allow both HTTP and HTTPS, so that CloudFront requires HTTPS for some objects but not for others.To implement this setup, you have to change the Origin Protocol Policy setting for the applicable origins in your distribution. If you\u2019re using the domain name that CloudFront assigned to your distribution, such as dtut0rial5d0j0.cloudfront.net, you change the Viewer Protocol Policy setting for one or more cache behaviors to require HTTPS communication. With this configuration, CloudFront provides the SSL/TLS certificate.","title":"Amazon CloudFront"},{"location":"aws-acd/tutorials-point/#amazon-s3","text":"Amazon S3 Cheat Sheet Server-side encryption protects data at rest. If you use Server-Side Encryption with Amazon S3-Managed Encryption Keys (SSE-S3), Amazon S3 will encrypt each object with a unique key and as an additional safeguard, it encrypts the key itself with a master key that it rotates regularly. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data. x-amz-server-side-encryption header to request server-side encryption. server-side encryption with customer-provided encryption keys (SSE-C), you must provide encryption key information using the following request headers: x-amz-server-side\u200b-encryption\u200b-customer-algorithm x-amz-server-side\u200b-encryption\u200b-customer-key x-amz-server-side\u200b-encryption\u200b-customer-key-MD5 since the put-bucket-policy command can only be used to apply policy at the bucket level, not on objects. You can use S3 Access Control Lists (ACLs) instead to manage permissions of S3 objects. To enable the CRR(cross-region replication) feature in S3, the following items should be met: The source and destination buckets must have versioning enabled. The source and destination buckets must be in different AWS Regions. Amazon S3 must have permissions to replicate objects from that source bucket to the destination bucket on your behalf.","title":"Amazon S3"},{"location":"aws-acd/tutorials-point/#aws-codecommit","text":"AWS CodeCommit Cheat Sheet In CodeCommit, the primary resource is a repository. Each resource has a unique Amazon Resource Names (ARN) associated with it. In a policy, you use an Amazon Resource Name (ARN) to identify the resource that the policy applies to. To manage access to AWS resources, you attach permissions policies to IAM identities. You use identity-based policies to control access to repositories. The codecommit:CreateRepository and codecommit:DeleteRepository permissions enable you to create and delete a CodeCommit repository respectively. Hence, these two permissions are the correct permissions that should be given to the technical manager. codecommit:GitPull and codecommit:GitPush permissions are incorrect because the codecommit:GitPull permission simply pulls information from a CodeCommit repository to a local repo and the codecommit:GitPush permission is just used to push information from a local repo to a CodeCommit repository. codecommit:CreateBranch and codecommit:DeleteBranch permissions are incorrect because these enable you to create and delete a specific branch and not a repository. For this scenario, you have to use the codecommit:CreateRepository and codecommit:DeleteRepository permissions instead. codecommit:* is incorrect because although this permission will allow all actions to the code repository. This entails a security risk and violates the standard security practice of granting only the permissions required to perform certain tasks.","title":"AWS CodeCommit"},{"location":"aws-acd/tutorials-point/#aws-codedeploy","text":"AWS CodeDeploy Cheat Sheet CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services. Canary deployment configuration, the traffic is shifted in two increments. You can choose from predefined canary options that specify the percentage of traffic shifted to your updated Lambda function version in the first increment and the interval, in minutes, before the remaining traffic is shifted in the second increment. Hence, this is the correct answer which will satisfy the requirement for the given scenario. Linear will cause the traffic to be shifted in equal increments with an equal number of minutes between each increment. You can choose from predefined linear options that specify the percentage of traffic shifted in each increment and the number of minutes between each increment. All-at-once with this deployment configuration, the traffic is shifted from the original Lambda function to the updated Lambda function version all at once. Rolling with additional batch this is only applicable in Elastic Beanstalk and not for Lambda. CodeDeploy provides two deployment type options: In-place deployment: The application on each instance in the deployment group is stopped, the latest application revision is installed, and the new version of the application is started and validated. You can use a load balancer so that each instance is deregistered during its deployment and then restored to service after the deployment is complete. Only deployments that use the EC2/On-Premises compute platform can use in-place deployments. AWS Lambda compute platform deployments cannot use an in-place deployment type. Blue/green deployment: The behavior of your deployment depends on which compute platform you use: \u2013 Blue/green on an EC2/On-Premises compute platform: The instances in a deployment group (the original environment) are replaced by a different set of instances (the replacement environment). If you use an EC2/On-Premises compute platform, be aware that blue/green deployments work with Amazon EC2 instances only. \u2013 Blue/green on an AWS Lambda compute platform: Traffic is shifted from your current serverless environment to one with your updated Lambda function versions. You can specify Lambda functions that perform validation tests and choose the way in which the traffic shift occurs. All AWS Lambda compute platform deployments are blue/green deployments. For this reason, you do not need to specify a deployment type. \u2013 Blue/green on an Amazon ECS compute platform: Traffic is shifted from the task set with the original version of a containerized application in an Amazon ECS service to a replacement task set in the same service. The protocol and port of a specified load balancer listener are used to reroute production traffic. During deployment, a test listener can be used to serve traffic to the replacement task set while validation tests are run. The CodeDeploy agent is a software package that, when installed and configured on an instance, makes it possible for that instance to be used in CodeDeploy deployments. The CodeDeploy agent communicates outbound using HTTPS over port 443. CodeDeploy agent is required only if you deploy to an EC2/On-Premises compute platform. The agent is not required for deployments that use the Amazon ECS or AWS Lambda compute platform.","title":"AWS CodeDeploy"},{"location":"aws-acd/tutorials-point/#amazon-dynamodb","text":"Amazon DynamoDB Cheat Sheet In general, Scan operations are less efficient than other operations in DynamoDB. A Scan operation always scans the entire table or secondary index. If possible, you should avoid using a Scan operation on a large table or index with a filter that removes many results. Also, as a table or index grows, the Scan operation slows. The Scan operation examines every item for the requested values and can use up the provisioned throughput for a large table or index in a single operation. For faster response times, design your tables and indexes so that your applications can use Query instead of Scan . For tables, you can also consider using the GetItem and BatchGetItem APIs. Time To Live (TTL) for DynamoDB allows you to define when items in a table expire so that they can be automatically deleted from the database.TTL is provided at no extra cost as a way to reduce storage usage and reduce the cost of storing irrelevant data without using provisioned throughput. With TTL enabled on a table, you can set a timestamp for deletion on a per-item basis, allowing you to limit storage usage to only those records that are relevant. DynamoDB Streams enables solutions such as these, and many others. DynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table, and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near real time.A DynamoDB stream is an ordered flow of information about changes to items in an Amazon DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. Amazon DynamoDB is also integrated with AWS Lambda so that you can create triggers\u2014pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables.When an item in the table is modified, StreamViewType determines what information are written to the stream for this table. Valid values for StreamViewType are KEYS_ONLY , NEW_IMAGE , OLD_IMAGE , and NEW_AND_OLD_IMAGES . OLD_IMAGE type, the entire item which has the previous value as it appeared before it was modified is written to the stream. KEYS_ONLY type, it will only write the key attributes of the modified item to the stream. NEW_IMAGE type, it will configure the stream to write the entire item with its new value as it appears after it was modified. NEW_AND_OLD_IMAGES type, it will send both the new and the old item images of the item to the stream Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement \u2013 from milliseconds to microseconds DynamoDB supports two types of secondary indexes: Global secondary index \u2014 an index with a partition key and a sort key that can be different from those on the base table. A global secondary index is considered \u201cglobal\u201d because queries on the index can span all of the data in the base table, across all partitions. Local secondary index \u2014 an index that has the same partition key as the base table, but a different sort key. A local secondary index is \u201clocal\u201d in the sense that every partition of a local secondary index is scoped to a base table partition that has the same partition key value. atomic counter \u2014 a numeric attribute that is incremented, unconditionally, without interfering with other write requests. (All write requests are applied in the order in which they were received). With an atomic counter, the updates are not idempotent. In other words, the numeric value will increment each time you call UpdateItem. conditional update - where overcounting or undercounting cannot be tolerated (For example, in a banking application). In this case, it is safer to use a conditional update instead of an atomic counter. To return the number of write capacity units consumed by any of these operations, set the ReturnConsumedCapacity parameter to one of the following: TOTAL \u2014 returns the total number of write capacity units consumed. INDEXES \u2014 returns the total number of write capacity units consumed, with subtotals for the table and any secondary indexes that were affected by the operation. NONE \u2014 no write capacity details are returned. (This is the default.) Local Secondary Index((LSI) - You cannot add a local secondary index to an existing table, nor can you delete any local secondary indexes that currently exist. Global Secondary Index (GSI) - You could add global secondary index to an existing table. Local secondary indexes are created at the same time that you create a table. You cannot add a local secondary index to an existing table, nor can you delete any local secondary indexes that currently exist. Global Secondary Index vs. Local Secondary Index","title":"Amazon DynamoDB"},{"location":"aws-acd/tutorials-point/#aws-elastic-beanstalk","text":"AWS Elastic Beanstalk Cheat Sheets In ElasticBeanstalk, you can choose from a variety of deployment methods: All at once \u2013 Deploy the new version to all instances simultaneously. All instances in your environment are out of service for a short time while the deployment occurs. Rolling \u2013 Deploy the new version in batches. Each batch is taken out of service during the deployment phase, reducing your environment\u2019s capacity by the number of instances in a batch. Rolling with additional batch \u2013 Deploy the new version in batches, but first launch a new batch of instances to ensure full capacity during the deployment process. Immutable \u2013 Deploy the new version to a fresh group of instances by performing an immutable update. Blue/Green \u2013 Deploy the new version to a separate environment, and then swap CNAMEs of the two environments to redirect traffic to the new version instantly. In Elastic Beanstalk, env.yaml : you can include a YAML formatted environment manifest in the root of your application source bundle to configure the environment name, solution stack and environment links to use when creating your environment. An environment manifest uses the same format as Saved Configurations. Dockerrun.aws.json :this configuration file is primarily used in multicontainer Docker environments that are hosted in Elastic Beanstalk. This can be used alone or combined with source code and content in a source bundle to create an environment on a Docker platform. env.config : this is just a custom configuration file which is not readily available in Elastic Beanstalk. Configuration files are YAML- or JSON-formatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle. The more appropriate configuration file to use here is the env.yaml which can help you configure the environment name, solution stack, and environment links to use when creating your environment. cron.yaml : this configuration file is primarily used to define periodic tasks that add jobs to your worker environment\u2019s queue automatically at a regular interval.","title":"AWS Elastic Beanstalk"},{"location":"aws-acd/tutorials-point/#amazon-ecs","text":"Amazon ECS Cheat Sheet A task placement strategy is an algorithm for selecting instances for task placement or tasks for termination. Task placement strategies can be specified when either running a task or creating a new service.Amazon ECS supports the following task placement strategies: binpack \u2013 Place tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use. random \u2013 Place tasks randomly. spread \u2013 Place tasks evenly based on the specified value. Accepted values are attribute key-value pairs, instanceId, or host. The Random task placement strategy is fairly straightforward as it doesn\u2019t require further parameters. The two other strategies, such as binpack and spread , take opposite actions. Binpack places tasks on as few instances as possible, helping to optimize resource utilization, while spread places tasks evenly across your cluster to help maximize availability. By default, ECS uses spread with the ecs.availability-zone attribute to place tasks. Random places tasks on instances at random yet still honors the other constraints that you specified, implicitly or explicitly. Specifically, it still makes sure that tasks are scheduled on instances with enough resources to run them. Port mappings allow containers to access ports on the host container instance to send or receive traffic. Port mappings are specified as part of the container definition which can be configured in the task definition.For task definitions that use the awsvpc network mode, you should only specify the containerPort . The hostPort can be left blank or it must be the same value as the containerPort .","title":"Amazon ECS"},{"location":"aws-acd/tutorials-point/#aws-serverless-application-model-sam","text":"A serverless application can include one or more nested applications. AWS::Serverless::Application \u2013 define a nested application in your serverless application. AWS::Serverless::Function \u2013 describes configuration information for creating a Lambda function. AWS::Serverless::LayerVersion \u2013 creates a Lambda layer version (LayerVersion) AWS::Serverless::Api \u2013 describes an API Gateway resource","title":"AWS (Serverless Application Model) SAM"},{"location":"aws-acd/tutorials-point/#amazon-elasticache","text":"Amazon Elasticache Cheat Sheet Amazon ElastiCache is an in-memory key/value store that sits between your application and the data store (database) that it accesses. Whenever your application requests data, it first makes the request to the ElastiCache cache. Lazy Loading as its name implies, is a caching strategy that loads data into the cache only when necessary. Write Through This strategy adds data or updates data in the cache whenever data are written to the database, and not on the event of a cache miss. Adding TTL just adds a time to live (TTL) value to each write operation, which will eventually expire the cache object.","title":"Amazon Elasticache"},{"location":"aws-acd/tutorials-point/#amazon-kms","text":"The GenerateDataKeyWithoutPlaintext API generates a unique data key. This operation returns a data key that is encrypted under a customer master key (CMK) that you specify. GenerateDataKeyWithoutPlaintext is identical to GenerateDataKey except that it returns only the encrypted copy of the data key. GenerateDataKey this operation also returns a plaintext copy of the data key along with the copy of the encrypted data key under a customer master key (CMK) that you specified. Take note that the scenario explicitly mentioned that the API must return only the encrypted copy of the data key which will be used later for encryption. Although this API can be used in this scenario, it is not recommended since the actual encryption process of the data happens at a later time and not in real-time.","title":"Amazon KMS"},{"location":"aws-acd/tutorials-point/#amazon-sqs","text":"Amazon SQS Cheat Sheet Amazon SQS long polling is a way to retrieve messages from your Amazon SQS queues. While the regular short polling returns immediately, even if the message queue being polled is empty, long polling doesn\u2019t return a response until a message arrives in the message queue, or the long poll times out.Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren\u2019t included in a response). This type of polling is suitable if the new messages that are being added to the SQS queue arrive less frequently.","title":"Amazon SQS"},{"location":"aws-acd/tutorials-point/#amazon-sns","text":"Amazon SNS Cheat Sheet Amazon Simple Notification Service (Amazon SNS) is a web service that coordinates and manages the delivery or sending of messages to subscribing endpoints or clients.With Amazon SNS, you have the ability to send push notification messages directly to apps on mobile devices. Push notification messages sent to a mobile endpoint can appear in the mobile app as message alerts, badge updates, or even sound alerts. SQS this service is mainly used for message queues and not for notifications. SES this service is mainly used for sending emails and not for notifications.","title":"Amazon SNS"},{"location":"aws-acd/tutorials-point/#amazon-kinesis","text":"Amazon Kinesis Cheat Sheet when you use the Kinesis Client Library (KCL), you should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. Amazon Kinesis Data Streams supports changes to the data record retention period of your stream. A Kinesis data stream is an ordered sequence of data records meant to be written to and read from in real time. Data records are therefore stored in shards in your stream temporarily. The time period from when a record is added to when it is no longer accessible is called the retention period. A Kinesis data stream stores records from 24 hours by default, up to 168 hours.","title":"Amazon Kinesis"},{"location":"aws-acd/tutorials-point/#aws-systems-manager","text":"AWS Systems Manager Cheat Sheet AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. You can store values as plain text or encrypted data. You can then reference values by using the unique name that you specified when you created the parameter.","title":"AWS Systems Manager"},{"location":"aws-acd/tutorials-point/#aws-secrets-manager","text":"AWS Secrets Manager Cheat Sheet you can configure Secrets Manager to automatically rotate the secret for you according to a schedule that you specify.","title":"AWS Secrets Manager"},{"location":"aws-acd/tutorials-point/#aws-iam","text":"AWS IAM Cheat Sheet Below is the summary of the available STS API: AssumeRole \u2013 is useful for allowing existing IAM users to access AWS resources that they don\u2019t already have access to. For example, the user might need access to resources in another AWS account. It is also useful as a means to temporarily gain privileged access\u2014for example, to provide multi-factor authentication (MFA). You must call this API using existing IAM user credentials. AssumeRoleWithWebIdentity \u2013 returns a set of temporary security credentials for federated users who are authenticated through a public identity provider. Examples of public identity providers include Login with Amazon, Facebook, Google, or any OpenID Connect (OIDC)-compatible identity provider. AssumeRoleWithSAML \u2013 returns a set of temporary security credentials for federated users who are authenticated by your organization\u2019s existing identity system. The users must also use SAML 2.0 (Security Assertion Markup Language) to pass authentication and authorization information to AWS. This API operation is useful in organizations that have integrated their identity systems (such as Windows Active Directory or OpenLDAP) with software that can produce SAML assertions. GetFederationToken \u2013 returns a set of temporary security credentials (consisting of an access key ID, a secret access key, and a security token) for a federated user. A typical use is in a proxy application that gets temporary security credentials on behalf of distributed applications inside a corporate network. You must call the GetFederationToken operation using the long-term security credentials of an IAM user. GetSessionToken \u2013 returns a set of temporary security credentials to an existing IAM user. This is useful for providing enhanced security, such as allowing AWS requests only when MFA is enabled for the IAM user. Because the credentials are temporary, they provide enhanced security when you have an IAM user who accesses your resources through a less secure environment.","title":"AWS IAM"},{"location":"aws-acd/tutorials-point/#amazon-quicksight","text":"Amazon QuickSight Cheat Sheet Amazon QuickSight is a fast, cloud-powered business intelligence service that makes it easy to deliver insights to everyone in your organization. As a fully managed service, QuickSight lets you easily create and publish interactive dashboards that include ML Insights. Dashboards can then be accessed from any device and embedded into your applications, portals, and websites.","title":"Amazon QuickSight"},{"location":"aws-acd/tutorials-point/#amazon-s3-glacier","text":"Amazon S3 Glacier Cheat Sheet There are three options for retrieving data with varying access times and cost: Standard retrievals allow you to access any of your archives within several hours. Standard retrievals typically complete within 3\u20135 hours. This is the default option. Bulk retrievals are Glacier\u2019s lowest-cost retrieval option, which you can use to retrieve large amounts, even petabytes, of data inexpensively in a day. Bulk retrievals typically complete within 5\u201312 hours. Expedited retrievals allow you to quickly access your data when occasional urgent requests for a subset of archives are required. Expedited retrievals are typically made available within 1\u20135 minutes","title":"Amazon S3 Glacier"},{"location":"aws-acd/tutorials-point/#aws-elastic-load-balancing","text":"AWS Elastic Load Balancing Cheat Sheet Application Load Balancers provide two advanced options that you may want to configure when you use ALBs with AWS Lambda: support for multi-value headers health check configurations.","title":"AWS Elastic Load Balancing"},{"location":"aws-acd/tutorials-point/#aws-step-functions","text":"AWS Step Functions Cheat Sheet States can perform a variety of functions in your state machine: Task State \u2013 Do some work in your state machine Choice State \u2013 Make a choice between branches of execution Fail or Succeed State \u2013 Stop execution with failure or success Pass State \u2013 Simply pass its input to its output or inject some fixed data, without performing work. Wait State \u2013 Provide a delay for a certain amount of time or until a specified time/date. Parallel State \u2013 Begin parallel branches of execution. Map State \u2013 Dynamically iterate steps.","title":"AWS Step Functions"},{"location":"aws-acd/tutorials-point/#aws-s3-glacier","text":"Amazon S3 Glacier Cheat Sheet There are three options for retrieving data with varying access times and cost: Standard retrievals allow you to access any of your archives within several hours. Standard retrievals typically complete within 3\u20135 hours. This is the default option. Bulk retrievals are Glacier\u2019s lowest-cost retrieval option, which you can use to retrieve large amounts, even petabytes, of data inexpensively in a day. Bulk retrievals typically complete within 5\u201312 hours. Expedited retrievals allow you to quickly access your data when occasional urgent requests for a subset of archives are required. Expedited retrievals are typically made available within 1\u20135 minutes.","title":"AWS S3 Glacier"},{"location":"aws-acd/tutorials-point/#comparison-of-aws-services","text":"Comparison of AWS Services Cheat Sheets Redis can provide a much more durable and powerful cache layer to the prototype distributed system, however, you should take note of one keyword in the requirement: multithreaded . In terms of commands execution, Redis is mostly a single-threaded server . It is not designed to benefit from multiple CPU cores unlike Memcached, however, you can launch several Redis instances to scale out on several cores if needed. Memcached is a more suitable choice since the scenario specifies that the system will run large nodes with multiple cores or threads which Memcached can adequately provide. You can choose Memcached over Redis if you have the following requirements: You need the simplest model possible. You need to run large nodes with multiple cores or threads. You need the ability to scale out and in, adding and removing nodes as demand on your system increases and decreases. You need to cache objects, such as a database. The two main components of Amazon Cognito are user pools and identity pools. User pools are user directories that provide sign-up and sign-in options for your app users. Identity pools enable you to grant your users access to other AWS services. You can use identity pools and user pools separately or together. Amazon Cognito identity pools provide temporary AWS credentials for users who are guests (unauthenticated) and for users who have been authenticated and received a token. An identity pool is a store of user identity data specific to your account.Amazon Cognito identity pools enable you to create unique identities and assign permissions for users. Your identity pool can include: Users in an Amazon Cognito user pool Users who authenticate with external identity providers such as Facebook, Google, or a SAML-based identity provider Users authenticated via your own existing authentication process Databases employ locking mechanisms to ensure that data is always updated to the latest version and is concurrent. There are multiple types of locking strategies that benefit different use cases. Some of these are: Optimistic Locking: Optimistic locking is a strategy to ensure that the client-side item that you are updating (or deleting) is the same as the item in DynamoDB. If you use this strategy, then your database writes are protected from being overwritten by the writes of others \u2014 and vice-verse. Pessimistic Locking Overly Optimistic Locking","title":"Comparison of AWS Services"},{"location":"aws-acd/tutorials-point/#other-important-link","text":"Amazon Simple Workflow (SWF) vs AWS Step Functions vs Amazon SQS Comparison of AWS Services Cheat Sheets Kinesis Scaling, Resharding and Parallel Processing Redis (cluster mode enabled vs disabled) vs Memcached AWS Lambda Integration with Amazon DynamoDB Streams RDS vs DynamoDB DynamoDB Scan vs Query Elastic Beanstalk vs CloudFormation vs OpsWorks vs CodeDeploy","title":"Other Important Link"},{"location":"aws-acd/tutorials-point/#redis-vs-memcached","text":"You can choose Memcached over Redis if you have the following requirements: You need the simplest model possible. You need to run large nodes with multiple cores or threads. You need the ability to scale out and in, adding and removing nodes as demand on your system increases and decreases. You need to cache objects, such as a database.","title":"Redis vs Memcached"},{"location":"aws-acd/tutorials-point/#amazon-kinesis-data-firehose","text":"Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you\u2019re already using today.","title":"Amazon Kinesis Data Firehose"},{"location":"aws-acd/tutorials-point/#aws-step-functions_1","text":"AWS Step Functions is a web service that enables you to coordinate the components of distributed applications and microservices using visual workflows. You build applications from individual components that each perform a discrete function, or task, allowing you to scale and change applications quickly. The following are key features of AWS Step Functions: Step Functions is based on the concepts of tasks and state machines. You define state machines using the JSON-based Amazon States Language. The Step Functions console displays a graphical view of your state machine\u2019s structure. This provides a way to visually check your state machine\u2019s logic and monitor executions.","title":"AWS Step Functions"},{"location":"aws-saa/aws-saa-certification/","text":"AWS Solutions Architect Associate Crash Course \u00b6 Reference: Git Link Exam guide \u00b6 Link Resilience \u00b6 SLAs for common services, starting on page 54 Link SLAs for each service can be found here Link Single VPC design principles Link Serverless multi-tier architectures Link Route 53 health checks and DNS failover Link AWS global infrastructure Link Migrating resources from one region to another Link Performance \u00b6 EBS volume types with performance limits Link EFS performance Link Optimizing S3 performance Link RDS performance Link Performance best practices for DynamoDB Link Overview of caching services Link Example of scaling an infrastructure Link Great catalog of launchable solutions in AWS Link Security \u00b6 My AWS Security Specialty Cert video! Link AWS Organizations FAQ Link Creating WAF Web ACLs Link Using Secrets Manager for RDS credentials Link AWS Transit Gateway for connecting networks Link VPC security Link Cost Optimization \u00b6 Service pricing can be found as follows Link Operational Excellence \u00b6 Read the whole whitepaper Link","title":"Solutions Architecture resource"},{"location":"aws-saa/aws-saa-certification/#aws-solutions-architect-associate-crash-course","text":"Reference: Git Link","title":"AWS Solutions Architect Associate Crash Course"},{"location":"aws-saa/aws-saa-certification/#exam-guide","text":"Link","title":"Exam guide"},{"location":"aws-saa/aws-saa-certification/#resilience","text":"SLAs for common services, starting on page 54 Link SLAs for each service can be found here Link Single VPC design principles Link Serverless multi-tier architectures Link Route 53 health checks and DNS failover Link AWS global infrastructure Link Migrating resources from one region to another Link","title":"Resilience"},{"location":"aws-saa/aws-saa-certification/#performance","text":"EBS volume types with performance limits Link EFS performance Link Optimizing S3 performance Link RDS performance Link Performance best practices for DynamoDB Link Overview of caching services Link Example of scaling an infrastructure Link Great catalog of launchable solutions in AWS Link","title":"Performance"},{"location":"aws-saa/aws-saa-certification/#security","text":"My AWS Security Specialty Cert video! Link AWS Organizations FAQ Link Creating WAF Web ACLs Link Using Secrets Manager for RDS credentials Link AWS Transit Gateway for connecting networks Link VPC security Link","title":"Security"},{"location":"aws-saa/aws-saa-certification/#cost-optimization","text":"Service pricing can be found as follows Link","title":"Cost Optimization"},{"location":"aws-saa/aws-saa-certification/#operational-excellence","text":"Read the whole whitepaper Link","title":"Operational Excellence"},{"location":"cloud/core-concept/","text":"Kubernetes Core Concepts \u00b6 List most important concepts used in Kubernetes. Hardware \u00b6 Nodes \u00b6 Nodes are worker machines in Kubernetes, which can be any devices which has CPU and RAM memory. For example a node can be anything, from smart watches, smart phones, through laptops or even a RaspberryPi. When we work with cloud providers, a node is a virtual machine . So a node is an abstraction over a single device. The beatuy of this abstraction is that we don\u2019t need to know the underlying hardware structure, we will just use nodes, this way our infrastructure will be platform independent. Cluster \u00b6 The Cluster is a group of nodes . When you deploy programs onto the cluster, it automatically handles distribution of work to the individual nodes. If more resources are required (for example we need more memory) new nodes can be added to the cluster and the work will be redistributed automatically. We run our code on a cluster and we should\u2019t care on which node, the distribution of the work will be automatically handled. Persistent Volumes \u00b6 Because our code can be relocated from one node to another (for example a node doesn\u2019t have enough memory, so the work will be rescheduled on a different node with enough memory), data saved on a node is volatile. But there are cases when we want to save our data persistently. In this case we should use Persistent Volumes. A Persistent Volume is like an external hard drive, you can plug it in and save you data on it. Kubernetes was originally developed as a platform for stateless applications with persistent data stored elsewhere. As the project matured, many organizations wanted to also begin leveraging it for their stateful applications and so persistent volume management was added. Much like the early days of virtualization, database servers are not typically the first group of servers to move into this new architecture. The reason is that the database is the core of many applications and may contain valuable information so on-premises database systems still largely run in VMs or physical servers.","title":"k8s Core"},{"location":"cloud/core-concept/#kubernetes-core-concepts","text":"List most important concepts used in Kubernetes.","title":"Kubernetes Core Concepts"},{"location":"cloud/core-concept/#hardware","text":"","title":"Hardware"},{"location":"cloud/core-concept/#nodes","text":"Nodes are worker machines in Kubernetes, which can be any devices which has CPU and RAM memory. For example a node can be anything, from smart watches, smart phones, through laptops or even a RaspberryPi. When we work with cloud providers, a node is a virtual machine . So a node is an abstraction over a single device. The beatuy of this abstraction is that we don\u2019t need to know the underlying hardware structure, we will just use nodes, this way our infrastructure will be platform independent.","title":"Nodes"},{"location":"cloud/core-concept/#cluster","text":"The Cluster is a group of nodes . When you deploy programs onto the cluster, it automatically handles distribution of work to the individual nodes. If more resources are required (for example we need more memory) new nodes can be added to the cluster and the work will be redistributed automatically. We run our code on a cluster and we should\u2019t care on which node, the distribution of the work will be automatically handled.","title":"Cluster"},{"location":"cloud/core-concept/#persistent-volumes","text":"Because our code can be relocated from one node to another (for example a node doesn\u2019t have enough memory, so the work will be rescheduled on a different node with enough memory), data saved on a node is volatile. But there are cases when we want to save our data persistently. In this case we should use Persistent Volumes. A Persistent Volume is like an external hard drive, you can plug it in and save you data on it. Kubernetes was originally developed as a platform for stateless applications with persistent data stored elsewhere. As the project matured, many organizations wanted to also begin leveraging it for their stateful applications and so persistent volume management was added. Much like the early days of virtualization, database servers are not typically the first group of servers to move into this new architecture. The reason is that the database is the core of many applications and may contain valuable information so on-premises database systems still largely run in VMs or physical servers.","title":"Persistent Volumes"},{"location":"cloud/ingress-concept/","text":"Ingress Concept \u00b6","title":"Ingress"},{"location":"cloud/ingress-concept/#ingress-concept","text":"","title":"Ingress Concept"},{"location":"cloud/probe-concept/","text":"Kubernetes Probes \u00b6 Understanding Kubernetes Probes \u00b6 All the probe have the following parameters: initialDelaySeconds : number of seconds to wait before initiating liveness or readiness probes periodSeconds : how often to check the probe timeoutSeconds : number of seconds before marking the probe as timing out (failing the health check) successThreshold : minimum number of consecutive successful checks for the probe to pass failureThreshold : number of retries before marking the probe as failed. For liveness probes, this will lead to the pod restarting. For readiness probes, this will mark the pod as unready. Readiness Probes \u00b6 Readiness probes are used to let kubelet know when the application is ready to accept new traffic. If the application needs some time to initialize state after the process has started, configure the readiness probe to tell Kubernetes to wait before sending new traffic. A primary use case for readiness probes is directing traffic to deployments behind a service. Reference: GCP Blog Readiness probes is that it runs during the pod\u2019s entire lifecycle. This means that readiness probes will run not only at startup but repeatedly throughout as long as the pod is running. This is to deal with situations where the application is temporarily unavailable (i.e. loading large data, waiting on external connections). In this case, we don\u2019t want to necessarily kill the application but wait for it to recover. Readiness probes are used to detect this scenario and not send traffic to these pods until it passes the readiness check again. Liveness Probes \u00b6 Liveness probes are used to restart unhealthy containers. The kubelet periodically pings the liveness probe, determines the health, and kills the pod if it fails the liveness check. Liveness checks can help the application recover from a deadlock situation. Without liveness checks, Kubernetes deems a deadlocked pod healthy since the underlying process continues to run from Kubernetes\u2019s perspective. By configuring the liveness probe, the kubelet can detect that the application is in a bad state and restarts the pod to restore availability. Reference: GCP Blog & Image Credit: GCP Blog Startup Probes \u00b6 Startup probes are similar to readiness probes but only executed at startup. They are optimized for slow starting containers or applications with unpredictable initialization processes.","title":"Kubernetes Probe"},{"location":"cloud/probe-concept/#kubernetes-probes","text":"","title":"Kubernetes Probes"},{"location":"cloud/probe-concept/#understanding-kubernetes-probes","text":"All the probe have the following parameters: initialDelaySeconds : number of seconds to wait before initiating liveness or readiness probes periodSeconds : how often to check the probe timeoutSeconds : number of seconds before marking the probe as timing out (failing the health check) successThreshold : minimum number of consecutive successful checks for the probe to pass failureThreshold : number of retries before marking the probe as failed. For liveness probes, this will lead to the pod restarting. For readiness probes, this will mark the pod as unready.","title":"Understanding Kubernetes Probes"},{"location":"cloud/probe-concept/#readiness-probes","text":"Readiness probes are used to let kubelet know when the application is ready to accept new traffic. If the application needs some time to initialize state after the process has started, configure the readiness probe to tell Kubernetes to wait before sending new traffic. A primary use case for readiness probes is directing traffic to deployments behind a service. Reference: GCP Blog Readiness probes is that it runs during the pod\u2019s entire lifecycle. This means that readiness probes will run not only at startup but repeatedly throughout as long as the pod is running. This is to deal with situations where the application is temporarily unavailable (i.e. loading large data, waiting on external connections). In this case, we don\u2019t want to necessarily kill the application but wait for it to recover. Readiness probes are used to detect this scenario and not send traffic to these pods until it passes the readiness check again.","title":"Readiness Probes"},{"location":"cloud/probe-concept/#liveness-probes","text":"Liveness probes are used to restart unhealthy containers. The kubelet periodically pings the liveness probe, determines the health, and kills the pod if it fails the liveness check. Liveness checks can help the application recover from a deadlock situation. Without liveness checks, Kubernetes deems a deadlocked pod healthy since the underlying process continues to run from Kubernetes\u2019s perspective. By configuring the liveness probe, the kubelet can detect that the application is in a bad state and restarts the pod to restore availability. Reference: GCP Blog & Image Credit: GCP Blog","title":"Liveness Probes"},{"location":"cloud/probe-concept/#startup-probes","text":"Startup probes are similar to readiness probes but only executed at startup. They are optimized for slow starting containers or applications with unpredictable initialization processes.","title":"Startup Probes"},{"location":"openshift/openshift-concept/","text":"","title":"OpenShift Concept"},{"location":"useful-command/cloud-command/","text":"Useful Cloud command \u00b6 Essential Toolkit for kubectl \u00b6 1. kubectl get \u00b6 Use get to pull a list of resources you have currently on your cluster. The types of resources you can get include: Namespace Pod Node Deployment Service ReplicaSets kubectl get ns kubectl get pods -n kube-system kubectl get nodes kubectl get services 2. kubectl describe \u00b6 Describe shows the details of the resource you\u2019re looking at. Resources you can describe include: Nodes Pods Services Deployments Replica sets kubectl describe pods my-pods -n kube-system kubectl describe servcies my-services -n kube-system 3. kubectl logs \u00b6 logs offer detailed insights into what\u2019s happening inside Kubernetes in relation to the pod. kubectl logs my-pod -n kube-system 4. kubectl exec \u00b6 exec into a container to troubleshoot an application directly. kubectl exec - it my - pod - n kube - system / bin / bash 5. kubectl cp \u00b6 This command is for copying files and directories to and from containers, much like the Linux cp command. The syntax follows a kubectl cp format: kubectl cp commands_copy.txt charts/cherry-chart-88d49478c-dmcfv:commands.txt kubectl cheat sheet \u00b6 PDF link","title":"Cloud Related Command"},{"location":"useful-command/cloud-command/#useful-cloud-command","text":"","title":"Useful Cloud command"},{"location":"useful-command/cloud-command/#essential-toolkit-for-kubectl","text":"","title":"Essential Toolkit for kubectl"},{"location":"useful-command/cloud-command/#1-kubectl-get","text":"Use get to pull a list of resources you have currently on your cluster. The types of resources you can get include: Namespace Pod Node Deployment Service ReplicaSets kubectl get ns kubectl get pods -n kube-system kubectl get nodes kubectl get services","title":"1. kubectl get"},{"location":"useful-command/cloud-command/#2-kubectl-describe","text":"Describe shows the details of the resource you\u2019re looking at. Resources you can describe include: Nodes Pods Services Deployments Replica sets kubectl describe pods my-pods -n kube-system kubectl describe servcies my-services -n kube-system","title":"2. kubectl describe"},{"location":"useful-command/cloud-command/#3-kubectl-logs","text":"logs offer detailed insights into what\u2019s happening inside Kubernetes in relation to the pod. kubectl logs my-pod -n kube-system","title":"3. kubectl logs"},{"location":"useful-command/cloud-command/#4-kubectl-exec","text":"exec into a container to troubleshoot an application directly. kubectl exec - it my - pod - n kube - system / bin / bash","title":"4. kubectl exec"},{"location":"useful-command/cloud-command/#5-kubectl-cp","text":"This command is for copying files and directories to and from containers, much like the Linux cp command. The syntax follows a kubectl cp format: kubectl cp commands_copy.txt charts/cherry-chart-88d49478c-dmcfv:commands.txt","title":"5. kubectl cp"},{"location":"useful-command/cloud-command/#kubectl-cheat-sheet","text":"PDF link","title":"kubectl cheat sheet"},{"location":"useful-command/docker/","text":"Docker commands \u00b6 Essential Toolkit for docker \u00b6 Details check docker docs 1. version \u00b6 docker --version 2. download images \u00b6 Pull an image or a repository from a registry. docker pull [OPTIONS] NAME[:TAG|@DIGEST] # pull from docker hub docker pull ubuntu: 14.04 # Pull from a different registry docker pull myregistry . local: 5000 / testing / test - image 3. list images \u00b6 List all the docker images pulled on the system with image details docker images 4. build image \u00b6 Build an image from a Dockerfile. docker build [OPTIONS] PATH | URL | - # Build with PATH docker build . # Tag an image with (-t) and Specify a Dockerfile with (-f) docker build -f Dockerfile.debug -t vieux/apache:2.0 . 5. Run image \u00b6 Run the docker image mentioned in the command. docker run [OPTIONS] IMAGE[:TAG|@DIGEST] [COMMAND] [ARG\u2026] # run detached docker run -d -p 80:80 my_image 6. List containers \u00b6 lists all the docker containers are running with container details. docker ps # List all the docker containers running/exited/stopped with container details. docker ps -a 7. Access container \u00b6 Access the docker container and run commands inside the container. docker exec - it my_image bash 8. Removing container \u00b6 Remove the docker container with container id mentioned in the command. docker rm my-container 8. Removing image \u00b6 Remove the docker image with the docker image id mentioned in the command. docker rmi my-image 9. start Docker || Stop Docker \u00b6 Start or Stop the docker container with container id mentioned in the command. docker start my-container docker stop my-container docker restart my-container docker cheat sheet \u00b6 PDF link","title":"Docker Command"},{"location":"useful-command/docker/#docker-commands","text":"","title":"Docker commands"},{"location":"useful-command/docker/#essential-toolkit-for-docker","text":"Details check docker docs","title":"Essential Toolkit for docker"},{"location":"useful-command/docker/#1-version","text":"docker --version","title":"1. version"},{"location":"useful-command/docker/#2-download-images","text":"Pull an image or a repository from a registry. docker pull [OPTIONS] NAME[:TAG|@DIGEST] # pull from docker hub docker pull ubuntu: 14.04 # Pull from a different registry docker pull myregistry . local: 5000 / testing / test - image","title":"2. download images"},{"location":"useful-command/docker/#3-list-images","text":"List all the docker images pulled on the system with image details docker images","title":"3. list images"},{"location":"useful-command/docker/#4-build-image","text":"Build an image from a Dockerfile. docker build [OPTIONS] PATH | URL | - # Build with PATH docker build . # Tag an image with (-t) and Specify a Dockerfile with (-f) docker build -f Dockerfile.debug -t vieux/apache:2.0 .","title":"4. build image"},{"location":"useful-command/docker/#5-run-image","text":"Run the docker image mentioned in the command. docker run [OPTIONS] IMAGE[:TAG|@DIGEST] [COMMAND] [ARG\u2026] # run detached docker run -d -p 80:80 my_image","title":"5. Run image"},{"location":"useful-command/docker/#6-list-containers","text":"lists all the docker containers are running with container details. docker ps # List all the docker containers running/exited/stopped with container details. docker ps -a","title":"6. List containers"},{"location":"useful-command/docker/#7-access-container","text":"Access the docker container and run commands inside the container. docker exec - it my_image bash","title":"7. Access container"},{"location":"useful-command/docker/#8-removing-container","text":"Remove the docker container with container id mentioned in the command. docker rm my-container","title":"8. Removing container"},{"location":"useful-command/docker/#8-removing-image","text":"Remove the docker image with the docker image id mentioned in the command. docker rmi my-image","title":"8. Removing image"},{"location":"useful-command/docker/#9-start-docker-stop-docker","text":"Start or Stop the docker container with container id mentioned in the command. docker start my-container docker stop my-container docker restart my-container","title":"9. start Docker || Stop Docker"},{"location":"useful-command/docker/#docker-cheat-sheet","text":"PDF link","title":"docker cheat sheet"},{"location":"useful-command/helm/","text":"Helm chart commands \u00b6 Essential Toolkit for helm \u00b6 1. version \u00b6 helm version 2. List installed releases \u00b6 helm list helm get values <release> 3. install package \u00b6 helm install -f ./override.yaml <name> <chart> [--namespace <ns>] helm install mychart-0.1.0.tgz --dry-run --debug # Test installing 4. upgrading releases \u00b6 helm upgrade < name > [ -- namespace < ns > ] helm upgrade -- wait < name > # Wait for pods to come up 5. delete release \u00b6 helm delete --purge <name> 6. get release \u00b6 helm get <deployment-name> 7. helm lint \u00b6 helm lint <chart-name>","title":"Helm command"},{"location":"useful-command/helm/#helm-chart-commands","text":"","title":"Helm chart commands"},{"location":"useful-command/helm/#essential-toolkit-for-helm","text":"","title":"Essential Toolkit for helm"},{"location":"useful-command/helm/#1-version","text":"helm version","title":"1. version"},{"location":"useful-command/helm/#2-list-installed-releases","text":"helm list helm get values <release>","title":"2. List installed releases"},{"location":"useful-command/helm/#3-install-package","text":"helm install -f ./override.yaml <name> <chart> [--namespace <ns>] helm install mychart-0.1.0.tgz --dry-run --debug # Test installing","title":"3. install package"},{"location":"useful-command/helm/#4-upgrading-releases","text":"helm upgrade < name > [ -- namespace < ns > ] helm upgrade -- wait < name > # Wait for pods to come up","title":"4. upgrading releases"},{"location":"useful-command/helm/#5-delete-release","text":"helm delete --purge <name>","title":"5. delete release"},{"location":"useful-command/helm/#6-get-release","text":"helm get <deployment-name>","title":"6. get release"},{"location":"useful-command/helm/#7-helm-lint","text":"helm lint <chart-name>","title":"7. helm lint"},{"location":"useful-command/linux-shell/","text":"Linux Shell Script \u00b6 Essential Toolkit \u00b6 1. alias \u00b6 The alias command lets you give your own name to a command or sequence of commands. You can then type your short name, and the shell will execute the command or sequence of commands for you. alias cls=clear # create alias allow to find process ID ( PID ) # then pipes them through grep command alias pf = \" ps -e | grep $1 \" 2. cat \u00b6 The cat command (short for \u201cconcatenate\u201d) lists the contents of files to the terminal window. cat .bash_logout With files longer than the number of lines in your terminal window, the text will whip past too fast for you to read. You can pipe the output from cat through less to make the process more manageable. With less you can scroll forward and backward through the file using the Up and Down Arrow keys, the PgUp and PgDn keys, and the Home and End keys. Type q to quit from less. cat .bashrc | less 3. chmod \u00b6 The chmod command sets the file permissions flags on a file or folder. The flags define who can read, write to or execute the file. #-owner|group|others -rwxrwxrwx If the first character is a - the item is a file, if it is a d the item is a directory. The rest of the string is three sets of three characters. From the left, the first three represent the file permissions of the owner , the middle three represent the file permissions of the group and the rightmost three characters represent the permissions for others . In each set, an r stands for read, a w stands for write, and an x stands for execute. One way to use chmod is to provide the permissions you wish to give to the owner, group, and others as a 3 digit number. The leftmost digit represents the owner. The middle digit represents the group. The rightmost digit represents the others. The digits you can use and what they represent are listed here: 0 : No permission 1 : Execute permission 2 : Write permission 3 : Write and execute permissions 4 : Read permission 5 : Read and execute permissions 6 : Read and write permissions 7 : Read , write and execute permissions chmod - R 765 example . txt 4. chown \u00b6 The chown command allows you to change the owner and group owner of a file. You can use chown to change the owner or group, or both of a file. You must provide the name of the owner and the group, separated by a : character. You will need to use sudo. To retain dave as the owner of the file but to set mary as the group owner, use this command: sudo chown dave:mary example.txt 5. curl \u00b6 The curl command is a tool to retrieve information and files from Uniform Resource Locators (URLs) or internet addresses. The curl command may not be provided as a standard part of your Linux distribution. Use apt-get to install this package onto your system if you\u2019re using Ubuntu or another Debian-based distribution. On other Linux distributions, use your Linux distribution\u2019s package management tool instead. sudo apt-get install curl This command retrieves the file for us. Note that you need to specify the name of the file to save it in, using the -o (output) option. If you do not do this, the contents of the file are scrolled rapidly in the terminal window but not saved to your computer. curl https://raw.githubusercontent.com/torvalds/linux/master/kernel/events/core.c -o core.c test web site request in container curl https://google.com 6. df \u00b6 The df command shows the size, used space, and available space on the mounted filesystems of your computer. Two of the most useful options are the -h (human readable) and -x (exclude) options. The human-readable option displays the sizes in Mb or Gb instead of in bytes. df -h -x squashfs 7. diff \u00b6 The diff command compares two text files and shows the differences between them. There are many options to tailor the display to your requirements. The -y (side by side) option shows the line differences side by side. The -w (width) option lets you specify the maximum line width to use to avoid wraparound lines. The two files are called alpha1.txt and alpha2.txt in this example. The \u2013suppress-common-lines prevents diff from listing the matching lines, letting you focus on the lines which have differences. diff -y -W 70 alpha1.txt alpha2.txt --suppress-common-lines 8. echo \u00b6 The echo command prints (echoes) a string of text to the terminal window. echo $USER echo $HOME echo $PATH 9. find \u00b6 Use the find command to track down files that you know exist if you can\u2019t remember where you put them. find . -name *ones* We do this using the -type option with the f parameter. The f parameter stands for files. find . -type f -name *ones* If you want the search to be case insensitive use the -iname (insensitive name) option. find . -iname *wild* 10. finger \u00b6 The finger command gives you a short dump of information about a user, including the time of the user\u2019s last login, the user\u2019s home directory, and the user account\u2019s full name. finger $USER 11. free \u00b6 The free command gives you a summary of the memory usage with your computer. It does this for both the main Random Access Memory (RAM) and swap memory. The -h (human) option is used to provide human-friendly numbers and units. Without this option, the figures are presented in bytes. free -h 12. grep \u00b6 The grep utility searches for lines which contain a search pattern. When we looked at the alias command, we used grep to search through the output of another program, ps . The grep command can also search the contents of files. Here we\u2019re searching for the word \u201ctrain\u201d in all text files in the current directory. grep train *.txt 13. groups \u00b6 The groups command tells you which groups a user is a member of. groups mary 14. gzip \u00b6 The gzip command compresses files. By default, it removes the original file and leaves you with the compressed version. To retain both the original and the compressed version, use the -k (keep) option. gzip -k core.c 15. head \u00b6 The head command gives you a listing of the first 10 lines of a file. If you want to see fewer or more lines, use the -n (number) option. In this example, we use head with its default of 10 lines. We then repeat the command asking for only five lines. head -core.c head -n 5 core.c 16. history \u00b6 The history command lists the commands you have previously issued on the command line. You can repeat any of the commands from your history by typing an exclamation point ! and the number of the command from the history list. history # pick the number !199 17. kill \u00b6 The kill command allows you to terminate a process from the command line. You do this by providing the process ID (PID) of the process to kill. Don\u2019t kill processes willy-nilly. You need to have a good reason to do so. In this example, we\u2019ll pretend the shutter program has locked up. # Find PID first ps -e | grep shutter. kill 1692 18. less \u00b6 The less command allows you to view files without opening an editor. It\u2019s faster to use, and there\u2019s no chance of you inadvertently modifying the file. With less you can scroll forward and backward through the file using the Up and Down Arrow keys, the PgUp and PgDn keys and the Home and End keys. Press the Q key to quit from less. less core.c You can also pipe the output from other commands into less. To see the output from ls for a listing of your entire hard drive, use the following command: ls -R / | less 19. ls \u00b6 more details check man page # use the - l ( long ) option ls - l # To use human - friendly file sizes include the - h ( human ) option ls - lh # To include hidden files use the - a ( all files ) option ls - lha 20. man \u00b6 The man command displays the \u201cman pages\u201d for a command in less . The man pages are the user manual for that command. Because man uses less to display the man pages, you can use the search capabilities of less. For example, to see the man pages for chown, use the following command: man chown 21. tree \u00b6 tree is a recursive directory listing program that produces a depth-indented listing of files. ``` # If you are using Apple OS X, type: brew install tree # Display the tree hierarchy of a directory tree -a ./GFG ## list files with their permissions tree -p ./GFG ``` 22. mkdir \u00b6 The mkdir command allows you to create new directories in the filesystem. mkdir invoces mkdir -p quotes/yearly/2019 23. mv \u00b6 The mv command allows you to move files and directories from directory to directory. It also allows you to rename files. mv ~/Documents/Ukulele/Apache.pdf . # rename file mv Apache.pdf The_Shadows_Apache.pdf 24. passwd \u00b6 The passwd command lets you change the password for a user. Just type passwd to change your own password. sudo passwd mary 25. ping \u00b6 The ping command lets you verify that you have network connectivity with another network device. ping 192 . 168 . 4 . 18 # ping to run for a specific number of ping attempts , use the - c ( count ) option ping - c 5 192 . 168 . 4 . 18 # To hear a ping use the - a ( audible ) option . ping - a 192 . 168 . 4 . 18 26. ps \u00b6 The ps command lists running processes. Using ps without any options causes it to list the processes running in the current shell. ps # to see all the processes related to a particular user ps -u dave | less # To see every process that is running, use the -e( every process) ps -e | less # Shows User, All Processes and Terminal Information ps aux 27. pwd \u00b6 Nice and simple, the pwd command prints the working directory (the current directory) from the root / directory. pwd 28. shutdown \u00b6 The shutdown command lets you shut down or reboot your Linux system. shutdown # shut down immediately shutdown now 29. SSH \u00b6 Use the ssh command to make a connection to a remote Linux computer and log into your account.To make a connection, you must provide your user name and the IP address or domain name of the remote computer. ssh mary @192.168.4.23 30. sudo \u00b6 The sudo command is required when performing actions that require root or superuser permissions, such as changing the password for another user. sudo passwd mary 31. tail \u00b6 The tail command gives you a listing of the last 10 lines of a file. If you want to see fewer or more lines, use the -n (number) option.we use tail with its default of 10 lines. We then repeat the command asking for only five lines. tail -n 5 core.c 32. tar \u00b6 With the tar command, you can create an archive file (also called a tarball) that can contain many other files. * -c (create) option * -v (verbose) option * -f (filename) option * -z (gzip) option * -j(bzip2) option * -x (extract) tar -cvf songs.tar Ukulele/ tar -cvzf songs.tar.gz Ukulele/ tar -cvjf songs.tar.bz2 Ukulele/ # extract files tar -xvf songs.tar tar -xvzf songs.tar.gz tar -xvjf songs.tar.bz2 33. whoami or w \u00b6 The w command lists the currently logged in users. w Use whoami to find out who you are logged in as or who is logged into an unmanned Linux terminal. whoami 34. top \u00b6 The top command shows you a real-time display of the data relating to your Linux machine. The top of the screen is a status summary. Detail check here top 35. uname \u00b6 You can obtain some system information regarding the Linux computer you\u2019re working on with the uname command. # see everything uname -a # Use the -s (kernel name) option to see the type of kernel. uname -s # Use the -r (kernel release) option to see the kernel release. uname -r # Use the -v (kernel version) option to see the kernel version. uname -v 36. hostname \u00b6 hostname used to either display or, with appropriate privileges, set the current host name of the system. The host name is used by many applications to identify the machine. hostname -s 37. du \u00b6 The Linux \u201cdu\u201d (Disk Usage) is a standard Unix/Linux command, used to check the information of disk usage of files and directories on a machine. # Using \u201c-h\u201d option with \u201cdu\u201d command provides results in \u201cHuman Readable Format\u201c du -h /home/tecmint # To get the summary of a grand total disk usage size of an directory use the option \u201c-s\u201d as follows. du -sh /home/tecmint # Using \u201c-a\u201d flag with \u201cdu\u201d command displays the disk usage of all the files and directories. du -a /home/tecmint","title":"Linux Shell"},{"location":"useful-command/linux-shell/#linux-shell-script","text":"","title":"Linux Shell Script"},{"location":"useful-command/linux-shell/#essential-toolkit","text":"","title":"Essential Toolkit"},{"location":"useful-command/linux-shell/#1-alias","text":"The alias command lets you give your own name to a command or sequence of commands. You can then type your short name, and the shell will execute the command or sequence of commands for you. alias cls=clear # create alias allow to find process ID ( PID ) # then pipes them through grep command alias pf = \" ps -e | grep $1 \"","title":"1. alias"},{"location":"useful-command/linux-shell/#2-cat","text":"The cat command (short for \u201cconcatenate\u201d) lists the contents of files to the terminal window. cat .bash_logout With files longer than the number of lines in your terminal window, the text will whip past too fast for you to read. You can pipe the output from cat through less to make the process more manageable. With less you can scroll forward and backward through the file using the Up and Down Arrow keys, the PgUp and PgDn keys, and the Home and End keys. Type q to quit from less. cat .bashrc | less","title":"2. cat"},{"location":"useful-command/linux-shell/#3-chmod","text":"The chmod command sets the file permissions flags on a file or folder. The flags define who can read, write to or execute the file. #-owner|group|others -rwxrwxrwx If the first character is a - the item is a file, if it is a d the item is a directory. The rest of the string is three sets of three characters. From the left, the first three represent the file permissions of the owner , the middle three represent the file permissions of the group and the rightmost three characters represent the permissions for others . In each set, an r stands for read, a w stands for write, and an x stands for execute. One way to use chmod is to provide the permissions you wish to give to the owner, group, and others as a 3 digit number. The leftmost digit represents the owner. The middle digit represents the group. The rightmost digit represents the others. The digits you can use and what they represent are listed here: 0 : No permission 1 : Execute permission 2 : Write permission 3 : Write and execute permissions 4 : Read permission 5 : Read and execute permissions 6 : Read and write permissions 7 : Read , write and execute permissions chmod - R 765 example . txt","title":"3. chmod"},{"location":"useful-command/linux-shell/#4-chown","text":"The chown command allows you to change the owner and group owner of a file. You can use chown to change the owner or group, or both of a file. You must provide the name of the owner and the group, separated by a : character. You will need to use sudo. To retain dave as the owner of the file but to set mary as the group owner, use this command: sudo chown dave:mary example.txt","title":"4. chown"},{"location":"useful-command/linux-shell/#5-curl","text":"The curl command is a tool to retrieve information and files from Uniform Resource Locators (URLs) or internet addresses. The curl command may not be provided as a standard part of your Linux distribution. Use apt-get to install this package onto your system if you\u2019re using Ubuntu or another Debian-based distribution. On other Linux distributions, use your Linux distribution\u2019s package management tool instead. sudo apt-get install curl This command retrieves the file for us. Note that you need to specify the name of the file to save it in, using the -o (output) option. If you do not do this, the contents of the file are scrolled rapidly in the terminal window but not saved to your computer. curl https://raw.githubusercontent.com/torvalds/linux/master/kernel/events/core.c -o core.c test web site request in container curl https://google.com","title":"5. curl"},{"location":"useful-command/linux-shell/#6-df","text":"The df command shows the size, used space, and available space on the mounted filesystems of your computer. Two of the most useful options are the -h (human readable) and -x (exclude) options. The human-readable option displays the sizes in Mb or Gb instead of in bytes. df -h -x squashfs","title":"6. df"},{"location":"useful-command/linux-shell/#7-diff","text":"The diff command compares two text files and shows the differences between them. There are many options to tailor the display to your requirements. The -y (side by side) option shows the line differences side by side. The -w (width) option lets you specify the maximum line width to use to avoid wraparound lines. The two files are called alpha1.txt and alpha2.txt in this example. The \u2013suppress-common-lines prevents diff from listing the matching lines, letting you focus on the lines which have differences. diff -y -W 70 alpha1.txt alpha2.txt --suppress-common-lines","title":"7. diff"},{"location":"useful-command/linux-shell/#8-echo","text":"The echo command prints (echoes) a string of text to the terminal window. echo $USER echo $HOME echo $PATH","title":"8. echo"},{"location":"useful-command/linux-shell/#9-find","text":"Use the find command to track down files that you know exist if you can\u2019t remember where you put them. find . -name *ones* We do this using the -type option with the f parameter. The f parameter stands for files. find . -type f -name *ones* If you want the search to be case insensitive use the -iname (insensitive name) option. find . -iname *wild*","title":"9. find"},{"location":"useful-command/linux-shell/#10-finger","text":"The finger command gives you a short dump of information about a user, including the time of the user\u2019s last login, the user\u2019s home directory, and the user account\u2019s full name. finger $USER","title":"10. finger"},{"location":"useful-command/linux-shell/#11-free","text":"The free command gives you a summary of the memory usage with your computer. It does this for both the main Random Access Memory (RAM) and swap memory. The -h (human) option is used to provide human-friendly numbers and units. Without this option, the figures are presented in bytes. free -h","title":"11. free"},{"location":"useful-command/linux-shell/#12-grep","text":"The grep utility searches for lines which contain a search pattern. When we looked at the alias command, we used grep to search through the output of another program, ps . The grep command can also search the contents of files. Here we\u2019re searching for the word \u201ctrain\u201d in all text files in the current directory. grep train *.txt","title":"12. grep"},{"location":"useful-command/linux-shell/#13-groups","text":"The groups command tells you which groups a user is a member of. groups mary","title":"13. groups"},{"location":"useful-command/linux-shell/#14-gzip","text":"The gzip command compresses files. By default, it removes the original file and leaves you with the compressed version. To retain both the original and the compressed version, use the -k (keep) option. gzip -k core.c","title":"14. gzip"},{"location":"useful-command/linux-shell/#15-head","text":"The head command gives you a listing of the first 10 lines of a file. If you want to see fewer or more lines, use the -n (number) option. In this example, we use head with its default of 10 lines. We then repeat the command asking for only five lines. head -core.c head -n 5 core.c","title":"15. head"},{"location":"useful-command/linux-shell/#16-history","text":"The history command lists the commands you have previously issued on the command line. You can repeat any of the commands from your history by typing an exclamation point ! and the number of the command from the history list. history # pick the number !199","title":"16. history"},{"location":"useful-command/linux-shell/#17-kill","text":"The kill command allows you to terminate a process from the command line. You do this by providing the process ID (PID) of the process to kill. Don\u2019t kill processes willy-nilly. You need to have a good reason to do so. In this example, we\u2019ll pretend the shutter program has locked up. # Find PID first ps -e | grep shutter. kill 1692","title":"17. kill"},{"location":"useful-command/linux-shell/#18-less","text":"The less command allows you to view files without opening an editor. It\u2019s faster to use, and there\u2019s no chance of you inadvertently modifying the file. With less you can scroll forward and backward through the file using the Up and Down Arrow keys, the PgUp and PgDn keys and the Home and End keys. Press the Q key to quit from less. less core.c You can also pipe the output from other commands into less. To see the output from ls for a listing of your entire hard drive, use the following command: ls -R / | less","title":"18. less"},{"location":"useful-command/linux-shell/#19-ls","text":"more details check man page # use the - l ( long ) option ls - l # To use human - friendly file sizes include the - h ( human ) option ls - lh # To include hidden files use the - a ( all files ) option ls - lha","title":"19. ls"},{"location":"useful-command/linux-shell/#20-man","text":"The man command displays the \u201cman pages\u201d for a command in less . The man pages are the user manual for that command. Because man uses less to display the man pages, you can use the search capabilities of less. For example, to see the man pages for chown, use the following command: man chown","title":"20. man"},{"location":"useful-command/linux-shell/#21-tree","text":"tree is a recursive directory listing program that produces a depth-indented listing of files. ``` # If you are using Apple OS X, type: brew install tree # Display the tree hierarchy of a directory tree -a ./GFG ## list files with their permissions tree -p ./GFG ```","title":"21. tree"},{"location":"useful-command/linux-shell/#22-mkdir","text":"The mkdir command allows you to create new directories in the filesystem. mkdir invoces mkdir -p quotes/yearly/2019","title":"22. mkdir"},{"location":"useful-command/linux-shell/#23-mv","text":"The mv command allows you to move files and directories from directory to directory. It also allows you to rename files. mv ~/Documents/Ukulele/Apache.pdf . # rename file mv Apache.pdf The_Shadows_Apache.pdf","title":"23. mv"},{"location":"useful-command/linux-shell/#24-passwd","text":"The passwd command lets you change the password for a user. Just type passwd to change your own password. sudo passwd mary","title":"24. passwd"},{"location":"useful-command/linux-shell/#25-ping","text":"The ping command lets you verify that you have network connectivity with another network device. ping 192 . 168 . 4 . 18 # ping to run for a specific number of ping attempts , use the - c ( count ) option ping - c 5 192 . 168 . 4 . 18 # To hear a ping use the - a ( audible ) option . ping - a 192 . 168 . 4 . 18","title":"25. ping"},{"location":"useful-command/linux-shell/#26-ps","text":"The ps command lists running processes. Using ps without any options causes it to list the processes running in the current shell. ps # to see all the processes related to a particular user ps -u dave | less # To see every process that is running, use the -e( every process) ps -e | less # Shows User, All Processes and Terminal Information ps aux","title":"26. ps"},{"location":"useful-command/linux-shell/#27-pwd","text":"Nice and simple, the pwd command prints the working directory (the current directory) from the root / directory. pwd","title":"27. pwd"},{"location":"useful-command/linux-shell/#28-shutdown","text":"The shutdown command lets you shut down or reboot your Linux system. shutdown # shut down immediately shutdown now","title":"28. shutdown"},{"location":"useful-command/linux-shell/#29-ssh","text":"Use the ssh command to make a connection to a remote Linux computer and log into your account.To make a connection, you must provide your user name and the IP address or domain name of the remote computer. ssh mary @192.168.4.23","title":"29. SSH"},{"location":"useful-command/linux-shell/#30-sudo","text":"The sudo command is required when performing actions that require root or superuser permissions, such as changing the password for another user. sudo passwd mary","title":"30. sudo"},{"location":"useful-command/linux-shell/#31-tail","text":"The tail command gives you a listing of the last 10 lines of a file. If you want to see fewer or more lines, use the -n (number) option.we use tail with its default of 10 lines. We then repeat the command asking for only five lines. tail -n 5 core.c","title":"31. tail"},{"location":"useful-command/linux-shell/#32-tar","text":"With the tar command, you can create an archive file (also called a tarball) that can contain many other files. * -c (create) option * -v (verbose) option * -f (filename) option * -z (gzip) option * -j(bzip2) option * -x (extract) tar -cvf songs.tar Ukulele/ tar -cvzf songs.tar.gz Ukulele/ tar -cvjf songs.tar.bz2 Ukulele/ # extract files tar -xvf songs.tar tar -xvzf songs.tar.gz tar -xvjf songs.tar.bz2","title":"32. tar"},{"location":"useful-command/linux-shell/#33-whoami-or-w","text":"The w command lists the currently logged in users. w Use whoami to find out who you are logged in as or who is logged into an unmanned Linux terminal. whoami","title":"33. whoami or w"},{"location":"useful-command/linux-shell/#34-top","text":"The top command shows you a real-time display of the data relating to your Linux machine. The top of the screen is a status summary. Detail check here top","title":"34. top"},{"location":"useful-command/linux-shell/#35-uname","text":"You can obtain some system information regarding the Linux computer you\u2019re working on with the uname command. # see everything uname -a # Use the -s (kernel name) option to see the type of kernel. uname -s # Use the -r (kernel release) option to see the kernel release. uname -r # Use the -v (kernel version) option to see the kernel version. uname -v","title":"35. uname"},{"location":"useful-command/linux-shell/#36-hostname","text":"hostname used to either display or, with appropriate privileges, set the current host name of the system. The host name is used by many applications to identify the machine. hostname -s","title":"36. hostname"},{"location":"useful-command/linux-shell/#37-du","text":"The Linux \u201cdu\u201d (Disk Usage) is a standard Unix/Linux command, used to check the information of disk usage of files and directories on a machine. # Using \u201c-h\u201d option with \u201cdu\u201d command provides results in \u201cHuman Readable Format\u201c du -h /home/tecmint # To get the summary of a grand total disk usage size of an directory use the option \u201c-s\u201d as follows. du -sh /home/tecmint # Using \u201c-a\u201d flag with \u201cdu\u201d command displays the disk usage of all the files and directories. du -a /home/tecmint","title":"37. du"}]}